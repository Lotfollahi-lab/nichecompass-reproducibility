{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "364a9ebc-3e3c-4645-9049-a34bd084c8a8",
   "metadata": {},
   "source": [
    "# expiMap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c55227-147e-417f-b0dd-bb0b7f322930",
   "metadata": {},
   "source": [
    "- **Creator**: Sebastian Birk (<sebastian.birk@helmholtz-munich.de>).\n",
    "- **Affiliation:** Helmholtz Munich, Institute of Computational Biology (ICB), Talavera-López Lab\n",
    "- **Date of Creation:** 05.01.2023\n",
    "- **Date of Last Modification:** 19.08.2023"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa669117-f347-4666-b112-8ea6669fd9e9",
   "metadata": {},
   "source": [
    "- The expiMap source code is available at https://github.com/theislab/scarches.\n",
    "- The corresponding preprint is \"Lotfollahi, M. et al. Biologically informed deep learning to infer gene program activity in single cells. bioRxiv 2022.02.05.479217 (2022) doi:10.1101/2022.02.05.479217\".\n",
    "- The workflow of this notebook follows the tutorial from https://scarches.readthedocs.io/en/latest/expimap_surgery_pipeline_basic.html.\n",
    "- We use a modified version of the NicheCompass gene program mask with only target genes as the gene program mask for expimap. The reasons are that it is relevant for cell communication, to improve comparability and since the expiMap method did not work well on this dataset with the reactome gene program used in the above cited tutorial.\n",
    "- The authors use raw counts as input to expiMap. Therefore, we also use raw counts (stored in adata.X)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7529cde5-be12-403b-a94c-07561774b86c",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faad87bd-fef5-4429-a175-d714c491ae76",
   "metadata": {},
   "source": [
    "### 1.1 Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7f93960-c759-424f-8cb2-1d8698acae2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank: 0] Global seed set to 0\n",
      "/home/aih/sebastian.birk/.local/lib/python3.9/site-packages/flax/struct.py:136: FutureWarning: jax.tree_util.register_keypaths is deprecated, and will be removed in a future release. Please use `register_pytree_with_keys()` instead.\n",
      "  jax.tree_util.register_keypaths(data_clz, keypaths)\n",
      "/home/aih/sebastian.birk/.local/lib/python3.9/site-packages/flax/struct.py:136: FutureWarning: jax.tree_util.register_keypaths is deprecated, and will be removed in a future release. Please use `register_pytree_with_keys()` instead.\n",
      "  jax.tree_util.register_keypaths(data_clz, keypaths)\n",
      "WARNING:root:mvTCR is not installed. To use mvTCR models, please install it first using \"pip install mvtcr\"\n",
      "WARNING:root:multigrate is not installed. To use multigrate models, please install it first using \"pip install multigrate\".\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "import gdown\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scanpy as sc\n",
    "import scarches as sca\n",
    "import scipy.sparse as sp\n",
    "import squidpy as sq\n",
    "from nichecompass.utils import (add_gps_from_gp_dict_to_adata,\n",
    "                                extract_gp_dict_from_mebocost_es_interactions,\n",
    "                                extract_gp_dict_from_nichenet_lrt_interactions,\n",
    "                                extract_gp_dict_from_omnipath_lr_interactions,\n",
    "                                filter_and_combine_gp_dict_gps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b5efa5-2052-4986-8ae5-89cfab018515",
   "metadata": {},
   "source": [
    "### 1.2 Define Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5c8b48a-ed5e-48b5-8c5c-c1de11493aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"expimap\"\n",
    "latent_key = f\"{model_name}_latent\"\n",
    "leiden_resolution = 0.5 # used for Leiden clustering of latent space\n",
    "random_seed = 0 # used for Leiden clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28adc110-0f41-4a71-9838-dc7f0687809a",
   "metadata": {},
   "source": [
    "### 1.3 Run Notebook Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "334b87ca-3387-4ba9-8567-84bc4754ff0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.set_figure_params(figsize=(6, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ab6b302-1c0b-4937-8624-40629ada2e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get time of notebook execution for timestamping saved artifacts\n",
    "now = datetime.now()\n",
    "current_timestamp = now.strftime(\"%d%m%Y_%H%M%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85538952-006b-4b0b-a50c-fe7445ce22e2",
   "metadata": {},
   "source": [
    "### 1.4 Configure Paths and Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ddcc49c-ba22-4155-acd5-05b5b810e091",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder_path = \"../../datasets/srt_data/gold/\"\n",
    "benchmarking_folder_path = \"../../artifacts/single_sample_method_benchmarking\"\n",
    "figure_folder_path = f\"../../figures\"\n",
    "gp_data_folder_path = \"../../datasets/gp_data\" # gene program data\n",
    "ga_data_folder_path = \"../../datasets/ga_data\" # gene annotation data\n",
    "\n",
    "# Create required directories\n",
    "os.makedirs(gp_data_folder_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d974cd00-eafa-4432-b172-fafc4058a619",
   "metadata": {},
   "source": [
    "## 2. expiMap Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8791f7bf-e9f3-4384-9cef-2d6719d2d1fd",
   "metadata": {},
   "source": [
    "### 2.1 Prepare Gene Program Mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d721fdf-088a-4726-a10c-df7105c967bc",
   "metadata": {},
   "source": [
    "#### 2.1.1 Mouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7dd3a336-6522-43f7-94c0-9eca6ddd489b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preparing the gene program mask...\n",
      "Number of gene programs before filtering and combining: 1818.\n",
      "Number of gene programs after filtering and combining: 1818.\n"
     ]
    }
   ],
   "source": [
    "species = \"mouse\"\n",
    "\n",
    "nichenet_lr_network_file_path = gp_data_folder_path + \\\n",
    "                                \"/nichenet_lr_network_v2_\" \\\n",
    "                                f\"{species}.csv\"\n",
    "nichenet_ligand_target_matrix_file_path = gp_data_folder_path + \\\n",
    "                                          \"/nichenet_ligand_target_matrix_\" \\\n",
    "                                          f\"v2_{species}.csv\"\n",
    "omnipath_lr_network_file_path = gp_data_folder_path + \\\n",
    "                                     \"/omnipath_lr_network.csv\"\n",
    "gene_orthologs_mapping_file_path = ga_data_folder_path + \\\n",
    "                                   \"/human_mouse_gene_orthologs.csv\"\n",
    "\n",
    "print(\"\\nPreparing the gene program mask...\")\n",
    "# OmniPath gene programs\n",
    "mouse_omnipath_gp_dict = extract_gp_dict_from_omnipath_lr_interactions(\n",
    "    species=species,\n",
    "    min_curation_effort=0,\n",
    "    load_from_disk=True,\n",
    "    save_to_disk=False,\n",
    "    lr_network_file_path=omnipath_lr_network_file_path,\n",
    "    gene_orthologs_mapping_file_path=gene_orthologs_mapping_file_path,\n",
    "    plot_gp_gene_count_distributions=False)\n",
    "\n",
    "# NicheNet gene programs\n",
    "mouse_nichenet_gp_dict = extract_gp_dict_from_nichenet_lrt_interactions(\n",
    "    species=species,\n",
    "    version=\"v2\",\n",
    "    keep_target_genes_ratio=1.0,\n",
    "    max_n_target_genes_per_gp=250,\n",
    "    load_from_disk=True,\n",
    "    save_to_disk=False,\n",
    "    lr_network_file_path=nichenet_lr_network_file_path,\n",
    "    ligand_target_matrix_file_path=nichenet_ligand_target_matrix_file_path,\n",
    "    gene_orthologs_mapping_file_path=gene_orthologs_mapping_file_path,\n",
    "    plot_gp_gene_count_distributions=False)\n",
    "\n",
    "# Combine gene programs into one dictionary\n",
    "mouse_combined_gp_dict = dict(mouse_omnipath_gp_dict)\n",
    "mouse_combined_gp_dict.update(mouse_nichenet_gp_dict)\n",
    "\n",
    "mouse_mebocost_gp_dict = extract_gp_dict_from_mebocost_es_interactions(\n",
    "    dir_path=f\"{gp_data_folder_path}/metabolite_enzyme_sensor_gps\",\n",
    "    species=species,\n",
    "    plot_gp_gene_count_distributions=False)\n",
    "\n",
    "mouse_combined_gp_dict.update(mouse_mebocost_gp_dict)\n",
    "    \n",
    "# Filter and combine gene programs\n",
    "mouse_combined_new_gp_dict = filter_and_combine_gp_dict_gps(\n",
    "    gp_dict=mouse_combined_gp_dict,\n",
    "    gp_filter_mode=\"subset\",\n",
    "    combine_overlap_gps=True,\n",
    "    overlap_thresh_source_genes=0.9,\n",
    "    overlap_thresh_target_genes=0.9,\n",
    "    overlap_thresh_genes=0.9,\n",
    "    verbose=False)\n",
    "\n",
    "print(\"Number of gene programs before filtering and combining: \"\n",
    "      f\"{len(mouse_combined_new_gp_dict)}.\")\n",
    "print(f\"Number of gene programs after filtering and combining: \"\n",
    "      f\"{len(mouse_combined_new_gp_dict)}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69401d81-1893-4a20-a70b-6623778bb797",
   "metadata": {},
   "source": [
    "#### 2.1.2 Human"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9460b8e-9247-44f9-9d85-54cb40c0f851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preparing the gene program mask...\n",
      "Number of gene programs before filtering and combining: 1691.\n",
      "Number of gene programs after filtering and combining: 1691.\n"
     ]
    }
   ],
   "source": [
    "species = \"human\"\n",
    "\n",
    "nichenet_lr_network_file_path = gp_data_folder_path + \\\n",
    "                                \"/nichenet_lr_network_v2_\" \\\n",
    "                                f\"{species}.csv\"\n",
    "nichenet_ligand_target_matrix_file_path = gp_data_folder_path + \\\n",
    "                                          \"/nichenet_ligand_target_matrix_\" \\\n",
    "                                          f\"v2_{species}.csv\"\n",
    "omnipath_lr_network_file_path = gp_data_folder_path + \\\n",
    "                                     \"/omnipath_lr_network.csv\"\n",
    "gene_orthologs_mapping_file_path = ga_data_folder_path + \\\n",
    "                                   \"/human_mouse_gene_orthologs.csv\"\n",
    "\n",
    "print(\"\\nPreparing the gene program mask...\")\n",
    "# OmniPath gene programs\n",
    "human_omnipath_gp_dict = extract_gp_dict_from_omnipath_lr_interactions(\n",
    "    species=species,\n",
    "    min_curation_effort=0,\n",
    "    load_from_disk=True,\n",
    "    save_to_disk=False,\n",
    "    lr_network_file_path=omnipath_lr_network_file_path,\n",
    "    gene_orthologs_mapping_file_path=gene_orthologs_mapping_file_path,\n",
    "    plot_gp_gene_count_distributions=False)\n",
    "\n",
    "# NicheNet gene programs\n",
    "human_nichenet_gp_dict = extract_gp_dict_from_nichenet_lrt_interactions(\n",
    "    species=species,\n",
    "    version=\"v2\",\n",
    "    keep_target_genes_ratio=1.0,\n",
    "    max_n_target_genes_per_gp=250,\n",
    "    load_from_disk=True,\n",
    "    save_to_disk=False,\n",
    "    lr_network_file_path=nichenet_lr_network_file_path,\n",
    "    ligand_target_matrix_file_path=nichenet_ligand_target_matrix_file_path,\n",
    "    gene_orthologs_mapping_file_path=gene_orthologs_mapping_file_path,\n",
    "    plot_gp_gene_count_distributions=False)\n",
    "\n",
    "# Combine gene programs into one dictionary\n",
    "human_combined_gp_dict = dict(human_omnipath_gp_dict)\n",
    "human_combined_gp_dict.update(human_nichenet_gp_dict)\n",
    "\n",
    "human_mebocost_gp_dict = extract_gp_dict_from_mebocost_es_interactions(\n",
    "    dir_path=f\"{gp_data_folder_path}/metabolite_enzyme_sensor_gps\",\n",
    "    species=species,\n",
    "    plot_gp_gene_count_distributions=False)\n",
    "\n",
    "human_combined_gp_dict.update(human_mebocost_gp_dict)\n",
    "    \n",
    "# Filter and combine gene programs\n",
    "human_combined_new_gp_dict = filter_and_combine_gp_dict_gps(\n",
    "    gp_dict=human_combined_gp_dict,\n",
    "    gp_filter_mode=\"subset\",\n",
    "    combine_overlap_gps=True,\n",
    "    overlap_thresh_source_genes=0.9,\n",
    "    overlap_thresh_target_genes=0.9,\n",
    "    overlap_thresh_genes=0.9,\n",
    "    verbose=False)\n",
    "\n",
    "print(\"Number of gene programs before filtering and combining: \"\n",
    "      f\"{len(human_combined_new_gp_dict)}.\")\n",
    "print(f\"Number of gene programs after filtering and combining: \"\n",
    "      f\"{len(human_combined_new_gp_dict)}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df82c18e-4444-4e82-88d9-afc843f5e480",
   "metadata": {},
   "source": [
    "### 2.2 Define Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ea39b0f-9c9a-459a-ba2e-c843802a8e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_expimap_models(dataset,\n",
    "                         gp_dict,\n",
    "                         cell_type_key,\n",
    "                         adata_new=None,\n",
    "                         n_start_run=1,\n",
    "                         n_end_run=8,\n",
    "                         n_neighbor_list=[4, 4, 8, 8, 12, 12, 16, 16],\n",
    "                         plot_latent_umaps: bool=False):\n",
    "    \n",
    "    # Configure figure folder path\n",
    "    dataset_figure_folder_path = f\"{figure_folder_path}/{dataset}/method_benchmarking/expimap/{current_timestamp}\"\n",
    "    os.makedirs(dataset_figure_folder_path, exist_ok=True)\n",
    "    \n",
    "    # Create new adata to store results from training runs in storage-efficient way\n",
    "    if adata_new is None:\n",
    "        adata_original = sc.read_h5ad(data_folder_path + f\"{dataset}.h5ad\")\n",
    "        adata_new = sc.AnnData(sp.csr_matrix(\n",
    "            (adata_original.shape[0], adata_original.shape[1]),\n",
    "            dtype=np.float32))\n",
    "        adata_new.var_names = adata_original.var_names\n",
    "        adata_new.obs_names = adata_original.obs_names\n",
    "        adata_new.obs[\"cell_type\"] = adata_original.obs[cell_type_key].values\n",
    "        adata_new.obsm[\"spatial\"] = adata_original.obsm[\"spatial\"]\n",
    "        del(adata_original)\n",
    "    \n",
    "    model_seeds = list(range(10))\n",
    "    for run_number, n_neighbors in zip(np.arange(n_start_run, n_end_run+1), n_neighbor_list):\n",
    "        # n_neighbors is here only used for the latent neighbor graph construction used for\n",
    "        # UMAP generation and clustering as expiMap is not a spatial method\n",
    "        \n",
    "        # Load data\n",
    "        adata = sc.read_h5ad(data_folder_path + f\"{dataset}.h5ad\")\n",
    "        \n",
    "        # Store raw counts in optimized format in adata.X\n",
    "        adata.layers[\"counts\"] = adata.layers[\"counts\"].tocsr()\n",
    "        adata.X = adata.layers[\"counts\"]\n",
    "        \n",
    "        adata.obs[\"batch\"] == \"batch1\"  \n",
    "        \n",
    "        # Add the gene program dictionary as binary masks to the adata for model training\n",
    "        # Use only target genes from the NicheCompass gene program mask\n",
    "        add_gps_from_gp_dict_to_adata(\n",
    "            gp_dict=gp_dict,\n",
    "            adata=adata,\n",
    "            genes_uppercase=True,\n",
    "            gp_targets_mask_key=\"I\",\n",
    "            gp_sources_mask_key=\"_\",\n",
    "            gp_names_key=\"terms\",\n",
    "            min_genes_per_gp=1,\n",
    "            min_source_genes_per_gp=0,\n",
    "            min_target_genes_per_gp=0,\n",
    "            max_genes_per_gp=None,\n",
    "            max_source_genes_per_gp=None,\n",
    "            max_target_genes_per_gp=None)\n",
    "\n",
    "        # Determine dimensionality of hidden encoder\n",
    "        n_hidden_encoder = len(adata.uns[\"terms\"])\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Initialize model\n",
    "        intr_cvae = sca.models.EXPIMAP(adata=adata,\n",
    "                                       condition_key=\"batch\",\n",
    "                                       hidden_layer_sizes=[256, 256, 256],\n",
    "                                       recon_loss=\"nb\")\n",
    "\n",
    "        # Train model\n",
    "        early_stopping_kwargs = {\n",
    "            \"early_stopping_metric\": \"val_unweighted_loss\",\n",
    "            \"threshold\": 0,\n",
    "            \"patience\": 50,\n",
    "            \"reduce_lr\": True,\n",
    "            \"lr_patience\": 13,\n",
    "            \"lr_factor\": 0.1}\n",
    "        intr_cvae.train(\n",
    "            n_epochs=400,\n",
    "            alpha_epoch_anneal=100,\n",
    "            alpha=0.7,\n",
    "            alpha_kl=0.5,\n",
    "            weight_decay=0.,\n",
    "            early_stopping_kwargs=early_stopping_kwargs,\n",
    "            use_early_stopping=True,\n",
    "            monitor_only_val=False,\n",
    "            seed=model_seeds[run_number-1])\n",
    "\n",
    "        # Store latent representation\n",
    "        adata.obsm[latent_key] = intr_cvae.get_latent(mean=False, only_active=True)\n",
    "        \n",
    "        # Measure time for model training\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "        hours, rem = divmod(elapsed_time, 3600)\n",
    "        minutes, seconds = divmod(rem, 60)\n",
    "        print(f\"Duration of model training in run {run_number}: {int(hours)} hours, {int(minutes)} minutes and {int(seconds)} seconds.\")\n",
    "        adata_new.uns[f\"{model_name}_model_training_duration_run{run_number}\"] = (\n",
    "            elapsed_time)\n",
    "\n",
    "        if plot_latent_umaps:\n",
    "            # Use expiMap latent space for UMAP generation\n",
    "            sc.pp.neighbors(adata,\n",
    "                            use_rep=latent_key,\n",
    "                            n_neighbors=n_neighbors)\n",
    "            sc.tl.umap(adata)\n",
    "            fig = sc.pl.umap(adata,\n",
    "                             color=[cell_type_key],\n",
    "                             title=\"Latent Space with Cell Types: expiMap\",\n",
    "                             return_fig=True)\n",
    "            fig.savefig(f\"{dataset_figure_folder_path}/latent_{model_name}\"\n",
    "                        f\"_cell_types_run{run_number}.png\",\n",
    "                        bbox_inches=\"tight\")\n",
    "\n",
    "            # Compute latent Leiden clustering\n",
    "            sc.tl.leiden(adata=adata,\n",
    "                         resolution=leiden_resolution,\n",
    "                         random_state=random_seed,\n",
    "                         key_added=f\"latent_{model_name}_leiden_{str(leiden_resolution)}\")\n",
    "\n",
    "            # Create subplot of latent Leiden cluster annotations in physical and latent space\n",
    "            fig, axs = plt.subplots(nrows=2, ncols=1, figsize=(6, 12))\n",
    "            title = fig.suptitle(t=\"Latent and Physical Space with Leiden Clusters: expiMap\")\n",
    "            sc.pl.umap(adata=adata,\n",
    "                       color=[f\"latent_{model_name}_leiden_{str(leiden_resolution)}\"],\n",
    "                       title=f\"Latent Space with Leiden Clusters\",\n",
    "                       ax=axs[0],\n",
    "                       show=False)\n",
    "            sq.pl.spatial_scatter(adata=adata,\n",
    "                                  color=[f\"latent_{model_name}_leiden_{str(leiden_resolution)}\"],\n",
    "                                  title=f\"Physical Space with Leiden Clusters\",\n",
    "                                  shape=None,\n",
    "                                  ax=axs[1])\n",
    "\n",
    "            # Create and position shared legend\n",
    "            handles, labels = axs[0].get_legend_handles_labels()\n",
    "            lgd = fig.legend(handles, labels, bbox_to_anchor=(1.25, 0.9185))\n",
    "            axs[0].get_legend().remove()\n",
    "            axs[1].get_legend().remove()\n",
    "\n",
    "            # Adjust, save and display plot\n",
    "            plt.subplots_adjust(wspace=0, hspace=0.2)\n",
    "            fig.savefig(f\"{dataset_figure_folder_path}/latent_physical_comparison_\"\n",
    "                        f\"{model_name}_run{run_number}.png\",\n",
    "                        bbox_extra_artists=(lgd, title),\n",
    "                        bbox_inches=\"tight\")\n",
    "            plt.show()\n",
    "\n",
    "        # Store latent representation\n",
    "        adata_new.obsm[latent_key + f\"_run{run_number}\"] = adata.obsm[latent_key]\n",
    "\n",
    "        # Store intermediate adata to disk\n",
    "        adata_new.write(f\"{benchmarking_folder_path}/{dataset}_{model_name}.h5ad\")\n",
    "\n",
    "    # Store final adata to disk\n",
    "    adata_new.write(f\"{benchmarking_folder_path}/{dataset}_{model_name}.h5ad\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f25415-e54e-4d2e-a6e3-bd6f3eef0d72",
   "metadata": {},
   "source": [
    "### 2.3 Train Models on Benchmarking Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7cd035-1727-4316-98f2-d8652f717699",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_expimap_models(dataset=\"seqfish_mouse_organogenesis_embryo2\",\n",
    "                     gp_dict=mouse_combined_new_gp_dict,\n",
    "                     cell_type_key=\"celltype_mapped_refined\",\n",
    "                     adata_new=None,\n",
    "                     n_start_run=1,\n",
    "                     n_end_run=8,\n",
    "                     n_neighbor_list=[4, 4, 8, 8, 12, 12, 16, 16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1659054d-fa4b-4e42-807f-1ec2e0fba87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for subsample_pct in [50, 25, 10, 5, 1]:\n",
    "    train_expimap_models(dataset=f\"seqfish_mouse_organogenesis_subsample_{subsample_pct}pct_embryo2\",\n",
    "                         gp_dict=mouse_combined_new_gp_dict,\n",
    "                         cell_type_key=\"celltype_mapped_refined\",\n",
    "                         adata_new=None,\n",
    "                         n_start_run=1,\n",
    "                         n_end_run=8,\n",
    "                         n_neighbor_list=[4, 4, 8, 8, 12, 12, 16, 16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47652164-0a56-4bd8-af0e-c39b061d318e",
   "metadata": {},
   "outputs": [],
   "source": [
    "workspace/projects/nichecompass-reproducibility/artifacts/single_sample_method_benchmarking/slideseqv2_mouse_hippocampus_expimap.h5ad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "372145e6-9b58-4efe-8556-599e4bb7359e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "INITIALIZING NEW NETWORK..............\n",
      "Encoder Architecture:\n",
      "\tInput Layer in, out and cond: 960 256 1\n",
      "\tHidden Layer 1 in/out: 256 256\n",
      "\tHidden Layer 2 in/out: 256 256\n",
      "\tMean/Var Layer in/out: 256 1494\n",
      "Decoder Architecture:\n",
      "\tMasked linear layer in, ext_m, ext, cond, out:  1494 0 0 1 960\n",
      "\twith hard mask.\n",
      "Last Decoder layer: softmax\n",
      "Preparing (77391, 960)\n",
      "Instantiating dataset\n",
      "Init the group lasso proximal operator for the main terms.\n",
      " |██████--------------| 30.2%  - epoch_loss: 444.1876587474 - epoch_recon_loss: 425.3543118258 - epoch_kl_loss: 37.6666934792 - val_loss: 520.8940559762 - val_recon_loss: 501.2428803991 - val_kl_loss: 39.3023515920233\n",
      "ADJUSTED LR\n",
      " |███████-------------| 37.5%  - epoch_loss: 444.6830586565 - epoch_recon_loss: 425.7556682062 - epoch_kl_loss: 37.8547818945 - val_loss: 520.0586057569 - val_recon_loss: 499.8348338643 - val_kl_loss: 40.4475452235\n",
      "ADJUSTED LR\n",
      " |████████------------| 40.8%  - epoch_loss: 442.8999136549 - epoch_recon_loss: 423.9920778047 - epoch_kl_loss: 37.8156718123 - val_loss: 519.8916075660 - val_recon_loss: 499.7247479548 - val_kl_loss: 40.3337167834\n",
      "ADJUSTED LR\n",
      " |████████------------| 44.0%  - epoch_loss: 445.9858607301 - epoch_recon_loss: 426.9758529243 - epoch_kl_loss: 38.0200158355 - val_loss: 520.0767662173 - val_recon_loss: 499.9029085753 - val_kl_loss: 40.3477202243\n",
      "ADJUSTED LR\n",
      " |█████████-----------| 46.8%  - epoch_loss: 443.8894867223 - epoch_recon_loss: 424.9468962223 - epoch_kl_loss: 37.8851813010 - val_loss: 519.7564242003 - val_recon_loss: 499.5703380147 - val_kl_loss: 40.3721725589\n",
      "Stopping early: no improvement of more than 0 nats in 50 epochs\n",
      "If the early stopping criterion is too strong, please instantiate it with different parameters in the train method.\n",
      "Saving best state of network...\n",
      "Best State was in Epoch 135\n",
      "Duration of model training in run 1: 0 hours, 30 minutes and 35 seconds.\n",
      "\n",
      "INITIALIZING NEW NETWORK..............\n",
      "Encoder Architecture:\n",
      "\tInput Layer in, out and cond: 960 256 1\n",
      "\tHidden Layer 1 in/out: 256 256\n",
      "\tHidden Layer 2 in/out: 256 256\n",
      "\tMean/Var Layer in/out: 256 1494\n",
      "Decoder Architecture:\n",
      "\tMasked linear layer in, ext_m, ext, cond, out:  1494 0 0 1 960\n",
      "\twith hard mask.\n",
      "Last Decoder layer: softmax\n",
      "Preparing (77391, 960)\n",
      "Instantiating dataset\n",
      "Init the group lasso proximal operator for the main terms.\n",
      " |██████--------------| 31.0%  - epoch_loss: 444.3947887176 - epoch_recon_loss: 425.9615891203 - epoch_kl_loss: 36.8663993066 - val_loss: 521.1671372711 - val_recon_loss: 501.4272050701 - val_kl_loss: 39.47986446449275\n",
      "ADJUSTED LR\n",
      " |███████-------------| 37.2%  - epoch_loss: 444.3972162929 - epoch_recon_loss: 425.7828723033 - epoch_kl_loss: 37.2286886023 - val_loss: 520.4635274918 - val_recon_loss: 500.6417676582 - val_kl_loss: 39.6435095990\n",
      "ADJUSTED LR\n",
      " |████████------------| 40.5%  - epoch_loss: 442.8097628812 - epoch_recon_loss: 424.2495304773 - epoch_kl_loss: 37.1204647379 - val_loss: 520.4038246030 - val_recon_loss: 500.5257003034 - val_kl_loss: 39.7562443467\n",
      "ADJUSTED LR\n",
      " |████████------------| 43.8%  - epoch_loss: 443.9520611964 - epoch_recon_loss: 425.3598640652 - epoch_kl_loss: 37.1843952144 - val_loss: 520.1843666952 - val_recon_loss: 500.3556458520 - val_kl_loss: 39.6574449383\n",
      "ADJUSTED LR\n",
      " |█████████-----------| 46.5%  - epoch_loss: 444.5581520011 - epoch_recon_loss: 425.9460165881 - epoch_kl_loss: 37.2242717498 - val_loss: 520.4744577877 - val_recon_loss: 500.6452851843 - val_kl_loss: 39.6583403916\n",
      "Stopping early: no improvement of more than 0 nats in 50 epochs\n",
      "If the early stopping criterion is too strong, please instantiate it with different parameters in the train method.\n",
      "Saving best state of network...\n",
      "Best State was in Epoch 134\n",
      "Duration of model training in run 2: 0 hours, 29 minutes and 30 seconds.\n",
      "\n",
      "INITIALIZING NEW NETWORK..............\n",
      "Encoder Architecture:\n",
      "\tInput Layer in, out and cond: 960 256 1\n",
      "\tHidden Layer 1 in/out: 256 256\n",
      "\tHidden Layer 2 in/out: 256 256\n",
      "\tMean/Var Layer in/out: 256 1494\n",
      "Decoder Architecture:\n",
      "\tMasked linear layer in, ext_m, ext, cond, out:  1494 0 0 1 960\n",
      "\twith hard mask.\n",
      "Last Decoder layer: softmax\n",
      "Preparing (77391, 960)\n",
      "Instantiating dataset\n",
      "Init the group lasso proximal operator for the main terms.\n",
      " |███████-------------| 38.0%  - epoch_loss: 444.0792248087 - epoch_recon_loss: 425.2950051964 - epoch_kl_loss: 37.5684393297 - val_loss: 520.6501554896 - val_recon_loss: 500.3974199139 - val_kl_loss: 40.5054632093111\n",
      "ADJUSTED LR\n",
      " |████████------------| 44.5%  - epoch_loss: 443.8732340681 - epoch_recon_loss: 424.9402814673 - epoch_kl_loss: 37.8659049638 - val_loss: 519.5778893643 - val_recon_loss: 499.3237594855 - val_kl_loss: 40.5082625092\n",
      "ADJUSTED LR\n",
      " |█████████-----------| 47.8%  - epoch_loss: 444.6254123513 - epoch_recon_loss: 425.6601375475 - epoch_kl_loss: 37.9305503495 - val_loss: 520.2332263384 - val_recon_loss: 499.9716706823 - val_kl_loss: 40.5231090608\n",
      "ADJUSTED LR\n",
      " |██████████----------| 51.0%  - epoch_loss: 444.4929638784 - epoch_recon_loss: 425.5546918677 - epoch_kl_loss: 37.8765445464 - val_loss: 519.4704854996 - val_recon_loss: 499.2286211858 - val_kl_loss: 40.4837285026\n",
      "ADJUSTED LR\n",
      " |██████████----------| 53.8%  - epoch_loss: 443.6054722217 - epoch_recon_loss: 424.6739662100 - epoch_kl_loss: 37.8630112184 - val_loss: 519.9585781410 - val_recon_loss: 499.7073344246 - val_kl_loss: 40.5024824299\n",
      "Stopping early: no improvement of more than 0 nats in 50 epochs\n",
      "If the early stopping criterion is too strong, please instantiate it with different parameters in the train method.\n",
      "Saving best state of network...\n",
      "Best State was in Epoch 163\n",
      "Duration of model training in run 3: 0 hours, 33 minutes and 48 seconds.\n"
     ]
    }
   ],
   "source": [
    "train_expimap_models(dataset=\"nanostring_cosmx_human_nsclc_batch5\",\n",
    "                     gp_dict=human_combined_new_gp_dict,\n",
    "                     cell_type_key=\"cell_type\",\n",
    "                     adata_new=adata_new,\n",
    "                     n_start_run=1,\n",
    "                     n_end_run=3,\n",
    "                     n_neighbor_list=[4, 4, 8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0db492dc-56da-4ae1-806d-dea051e5eb8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnnData object with n_obs × n_vars = 77391 × 960\n",
       "    obs: 'cell_type'\n",
       "    uns: 'expimap_model_training_duration_run4', 'expimap_model_training_duration_run5', 'expimap_model_training_duration_run6', 'expimap_model_training_duration_run7', 'expimap_model_training_duration_run8'\n",
       "    obsm: 'expimap_latent_run4', 'expimap_latent_run5', 'expimap_latent_run6', 'expimap_latent_run7', 'expimap_latent_run8', 'spatial'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adata_new"
   ]
  },
  {
   "cell_type": "raw",
   "id": "60151209-2fb4-457c-80b6-cbd6321050cd",
   "metadata": {},
   "source": [
    "for subsample_pct in [50]:\n",
    "    train_expimap_models(dataset=f\"vizgen_merfish_mouse_liver_subsample_{subsample_pct}pct\",\n",
    "                         cell_type_key=\"Cell_Type\",\n",
    "                         gp_dict=mouse_combined_new_gp_dict,\n",
    "                         adata_new=adata_new,\n",
    "                         n_start_run=2,\n",
    "                         n_end_run=8,\n",
    "                         n_neighbor_list=[4, 8, 8, 12, 12, 16, 16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0716d8a-fdc9-4308-950d-decc145cef40",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_expimap_models(dataset=\"nanostring_cosmx_human_nsclc_batch5\",\n",
    "                     gp_dict=human_combined_new_gp_dict,\n",
    "                     cell_type_key=\"cell_type\",\n",
    "                     adata_new=None,\n",
    "                     n_start_run=1,\n",
    "                     n_end_run=8,\n",
    "                     n_neighbor_list=[4, 4, 8, 8, 12, 12, 16, 16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b3d1af-a005-4790-bbd9-65e7c286ab1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for subsample_pct in [50, 25, 10, 5, 1]:\n",
    "    train_expimap_models(dataset=f\"nanostring_cosmx_human_nsclc_subsample_{subsample_pct}pct_batch5\",\n",
    "                         gp_dict=human_combined_new_gp_dict,\n",
    "                         cell_type_key=\"cell_type\",\n",
    "                         adata_new=None,\n",
    "                         n_start_run=1,\n",
    "                         n_end_run=8,\n",
    "                         n_neighbor_list=[4, 4, 8, 8, 12, 12, 16, 16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c63c2f3-38ca-476d-bf2e-cac68c30f9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_expimap_models(dataset=\"vizgen_merfish_mouse_liver\",\n",
    "                     gp_dict=mouse_combined_new_gp_dict,\n",
    "                     cell_type_key=\"Cell_Type\",\n",
    "                     adata_new=None,\n",
    "                     n_start_run=1,\n",
    "                     n_end_run=8,\n",
    "                     n_neighbor_list=[4, 4, 8, 8, 12, 12, 16, 16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2bfa53-80b3-4ba3-8da4-47b06443e731",
   "metadata": {},
   "outputs": [],
   "source": [
    "for subsample_pct in [50, 25, 10, 5, 1]:\n",
    "    train_expimap_models(dataset=f\"vizgen_merfish_mouse_liver_subsample_{subsample_pct}pct\",\n",
    "                         cell_type_key=\"Cell_Type\",\n",
    "                         gp_dict=mouse_combined_new_gp_dict,\n",
    "                         adata_new=None,\n",
    "                         n_start_run=1,\n",
    "                         n_end_run=8,\n",
    "                         n_neighbor_list=[4, 4, 8, 8, 12, 12, 16, 16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167e90f9-a124-4180-a142-f295f4ca7128",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_expimap_models(dataset=\"slideseqv2_mouse_hippocampus\",\n",
    "                     cell_type_key=\"cell_type\",\n",
    "                     gp_dict=mouse_combined_new_gp_dict,\n",
    "                     adata_new=None,\n",
    "                     n_start_run=1,\n",
    "                     n_end_run=8,\n",
    "                     n_neighbor_list=[4, 4, 8, 8, 12, 12, 16, 16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f389a8-62e6-4d1f-bcb6-2ded4e4d346c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "INITIALIZING NEW NETWORK..............\n",
      "Encoder Architecture:\n",
      "\tInput Layer in, out and cond: 4000 256 1\n",
      "\tHidden Layer 1 in/out: 256 256\n",
      "\tHidden Layer 2 in/out: 256 256\n",
      "\tMean/Var Layer in/out: 256 1519\n",
      "Decoder Architecture:\n",
      "\tMasked linear layer in, ext_m, ext, cond, out:  1519 0 0 1 4000\n",
      "\twith hard mask.\n",
      "Last Decoder layer: softmax\n",
      "Preparing (20885, 4000)\n",
      "Instantiating dataset\n",
      "Init the group lasso proximal operator for the main terms.\n",
      " |████████████--------| 60.0%  - epoch_loss: 245.1279332167 - epoch_recon_loss: 233.4520511757 - epoch_kl_loss: 23.3517622007 - val_loss: 181.8208250158 - val_recon_loss: 170.7912301456 - val_kl_loss: 22.05918558914639\n",
      "ADJUSTED LR\n",
      " |█████████████-------| 65.2%  - epoch_loss: 243.5904568004 - epoch_recon_loss: 231.9176900435 - epoch_kl_loss: 23.3455345258 - val_loss: 181.6455679501 - val_recon_loss: 170.4527489157 - val_kl_loss: 22.3856379565\n",
      "ADJUSTED LR\n",
      " |██████████████------| 73.0%  - epoch_loss: 245.8203151988 - epoch_recon_loss: 234.1535841754 - epoch_kl_loss: 23.3334618912 - val_loss: 182.5984622731 - val_recon_loss: 171.4675248090 - val_kl_loss: 22.2618745916\n",
      "ADJUSTED LR\n",
      " |███████████████-----| 76.2%  - epoch_loss: 244.1194405069 - epoch_recon_loss: 232.4164681986 - epoch_kl_loss: 23.4059437603 - val_loss: 181.9237724753 - val_recon_loss: 170.7842631621 - val_kl_loss: 22.2790208704\n",
      "ADJUSTED LR\n",
      " |███████████████-----| 79.5%  - epoch_loss: 243.7770517570 - epoch_recon_loss: 232.0874966991 - epoch_kl_loss: 23.3791087274 - val_loss: 181.8919749540 - val_recon_loss: 170.7497181612 - val_kl_loss: 22.2845116784\n",
      "ADJUSTED LR\n",
      " |████████████████----| 83.5%  - epoch_loss: 245.0098039277 - epoch_recon_loss: 233.3422893083 - epoch_kl_loss: 23.3350287587 - val_loss: 182.0548050824 - val_recon_loss: 170.8785355512 - val_kl_loss: 22.3525441114\n",
      "ADJUSTED LR\n",
      " |█████████████████---| 86.8%  - epoch_loss: 244.6148967094 - epoch_recon_loss: 232.9389770923 - epoch_kl_loss: 23.3518401555 - val_loss: 182.4269391228 - val_recon_loss: 171.2701039034 - val_kl_loss: 22.3136687559\n",
      "ADJUSTED LR\n",
      " |██████████████████--| 92.8%  - epoch_loss: 243.8934991538 - epoch_recon_loss: 232.2424160704 - epoch_kl_loss: 23.3021671788 - val_loss: 182.1614245246 - val_recon_loss: 171.0023274141 - val_kl_loss: 22.3181934357\n",
      "ADJUSTED LR\n",
      " |███████████████████-| 98.5%  - epoch_loss: 244.1391598448 - epoch_recon_loss: 232.4042709247 - epoch_kl_loss: 23.4697794493 - val_loss: 181.9806608312 - val_recon_loss: 170.8380342371 - val_kl_loss: 22.2852519540\n",
      "ADJUSTED LR\n",
      " |████████████████████| 100.0%  - epoch_loss: 243.9896013948 - epoch_recon_loss: 232.3072966491 - epoch_kl_loss: 23.3646107888 - val_loss: 181.5779050939 - val_recon_loss: 170.4466687371 - val_kl_loss: 22.2624720405\n",
      "Saving best state of network...\n",
      "Best State was in Epoch 379\n",
      "Duration of model training in run 1: 0 hours, 17 minutes and 59 seconds.\n",
      "\n",
      "INITIALIZING NEW NETWORK..............\n",
      "Encoder Architecture:\n",
      "\tInput Layer in, out and cond: 4000 256 1\n",
      "\tHidden Layer 1 in/out: 256 256\n",
      "\tHidden Layer 2 in/out: 256 256\n",
      "\tMean/Var Layer in/out: 256 1519\n",
      "Decoder Architecture:\n",
      "\tMasked linear layer in, ext_m, ext, cond, out:  1519 0 0 1 4000\n",
      "\twith hard mask.\n",
      "Last Decoder layer: softmax\n",
      "Preparing (20885, 4000)\n",
      "Instantiating dataset\n",
      "Init the group lasso proximal operator for the main terms.\n",
      " |████████████--------| 61.5%  - epoch_loss: 243.8052182360 - epoch_recon_loss: 232.4230802367 - epoch_kl_loss: 22.7642770495 - val_loss: 181.5883017147 - val_recon_loss: 170.6201566808 - val_kl_loss: 21.93628894585010\n",
      "ADJUSTED LR\n",
      " |█████████████-------| 67.2%  - epoch_loss: 244.3199350785 - epoch_recon_loss: 232.9031349234 - epoch_kl_loss: 22.8336004789 - val_loss: 181.7732184915 - val_recon_loss: 170.8670178582 - val_kl_loss: 21.8124009301\n",
      "ADJUSTED LR\n",
      " |███████████████-----| 75.2%  - epoch_loss: 244.9678338498 - epoch_recon_loss: 233.5148597769 - epoch_kl_loss: 22.9059472376 - val_loss: 181.4733114804 - val_recon_loss: 170.4450593836 - val_kl_loss: 22.0565023983\n",
      "ADJUSTED LR\n",
      " |███████████████-----| 78.5%  - epoch_loss: 244.5891032316 - epoch_recon_loss: 233.1787659522 - epoch_kl_loss: 22.8206753374 - val_loss: 181.9142824061 - val_recon_loss: 170.8773624196 - val_kl_loss: 22.0738389632\n",
      "ADJUSTED LR\n",
      " |████████████████----| 81.8%  - epoch_loss: 243.9231210306 - epoch_recon_loss: 232.5337573201 - epoch_kl_loss: 22.7787280699 - val_loss: 181.4693504782 - val_recon_loss: 170.4348126580 - val_kl_loss: 22.0690746307\n",
      "ADJUSTED LR\n",
      " |█████████████████---| 87.2%  - epoch_loss: 245.8399639519 - epoch_recon_loss: 234.3973692810 - epoch_kl_loss: 22.8851884336 - val_loss: 181.0494223202 - val_recon_loss: 170.0353187112 - val_kl_loss: 22.0282095741\n",
      "ADJUSTED LR\n",
      " |██████████████████--| 90.5%  - epoch_loss: 244.5735033515 - epoch_recon_loss: 233.2103706412 - epoch_kl_loss: 22.7262641362 - val_loss: 181.2971738928 - val_recon_loss: 170.2717285156 - val_kl_loss: 22.0508886225\n",
      "ADJUSTED LR\n",
      " |██████████████████--| 93.8%  - epoch_loss: 243.4109267669 - epoch_recon_loss: 231.9761421048 - epoch_kl_loss: 22.8695690518 - val_loss: 181.1651754940 - val_recon_loss: 170.1659366383 - val_kl_loss: 21.9984790578\n",
      "ADJUSTED LR\n",
      " |███████████████████-| 96.5%  - epoch_loss: 244.8203918042 - epoch_recon_loss: 233.4342979379 - epoch_kl_loss: 22.7721889652 - val_loss: 181.6682649500 - val_recon_loss: 170.6481906666 - val_kl_loss: 22.0401446399\n",
      "Stopping early: no improvement of more than 0 nats in 50 epochs\n",
      "If the early stopping criterion is too strong, please instantiate it with different parameters in the train method.\n",
      "Saving best state of network...\n",
      "Best State was in Epoch 334\n",
      "Duration of model training in run 2: 0 hours, 17 minutes and 19 seconds.\n",
      "\n",
      "INITIALIZING NEW NETWORK..............\n",
      "Encoder Architecture:\n",
      "\tInput Layer in, out and cond: 4000 256 1\n",
      "\tHidden Layer 1 in/out: 256 256\n",
      "\tHidden Layer 2 in/out: 256 256\n",
      "\tMean/Var Layer in/out: 256 1519\n",
      "Decoder Architecture:\n",
      "\tMasked linear layer in, ext_m, ext, cond, out:  1519 0 0 1 4000\n",
      "\twith hard mask.\n",
      "Last Decoder layer: softmax\n",
      "Preparing (20885, 4000)\n",
      "Instantiating dataset\n",
      "Init the group lasso proximal operator for the main terms.\n",
      " |███████████████████-| 95.0%  - epoch_loss: 242.6358863675 - epoch_recon_loss: 231.5119005061 - epoch_kl_loss: 22.2479710611 - val_loss: 177.9273421344 - val_recon_loss: 166.9159599753 - val_kl_loss: 22.02276207422889\n",
      "ADJUSTED LR\n",
      " |████████████████████| 100.0%  - epoch_loss: 243.6688929967 - epoch_recon_loss: 232.4625132035 - epoch_kl_loss: 22.4127600274 - val_loss: 177.9899668974 - val_recon_loss: 167.0217500574 - val_kl_loss: 21.9364334555\n",
      "Saving best state of network...\n",
      "Best State was in Epoch 387\n",
      "Duration of model training in run 3: 0 hours, 17 minutes and 58 seconds.\n",
      "\n",
      "INITIALIZING NEW NETWORK..............\n",
      "Encoder Architecture:\n",
      "\tInput Layer in, out and cond: 4000 256 1\n",
      "\tHidden Layer 1 in/out: 256 256\n",
      "\tHidden Layer 2 in/out: 256 256\n",
      "\tMean/Var Layer in/out: 256 1519\n",
      "Decoder Architecture:\n",
      "\tMasked linear layer in, ext_m, ext, cond, out:  1519 0 0 1 4000\n",
      "\twith hard mask.\n",
      "Last Decoder layer: softmax\n",
      "Preparing (20885, 4000)\n",
      "Instantiating dataset\n",
      "Init the group lasso proximal operator for the main terms.\n",
      " |████████████████----| 80.2%  - epoch_loss: 243.4714745762 - epoch_recon_loss: 232.3078490796 - epoch_kl_loss: 22.3272503185 - val_loss: 179.0997592702 - val_recon_loss: 168.2768330294 - val_kl_loss: 21.64585270605264\n",
      "ADJUSTED LR\n",
      " |█████████████████---| 85.5%  - epoch_loss: 243.4765680015 - epoch_recon_loss: 232.3090202695 - epoch_kl_loss: 22.3350956326 - val_loss: 179.2998791863 - val_recon_loss: 168.3876998004 - val_kl_loss: 21.8243585474\n",
      "ADJUSTED LR\n",
      " |██████████████████--| 91.5%  - epoch_loss: 243.3998329007 - epoch_recon_loss: 232.1751758809 - epoch_kl_loss: 22.4493148959 - val_loss: 179.2550722010 - val_recon_loss: 168.3649157356 - val_kl_loss: 21.7803118089\n",
      "ADJUSTED LR\n",
      " |██████████████████--| 94.8%  - epoch_loss: 243.0201572756 - epoch_recon_loss: 231.8328395506 - epoch_kl_loss: 22.3746358651 - val_loss: 179.2333795884 - val_recon_loss: 168.3533549589 - val_kl_loss: 21.7600494834\n",
      "ADJUSTED LR\n",
      " |████████████████████| 100.0%  - epoch_loss: 243.5834612165 - epoch_recon_loss: 232.3858451584 - epoch_kl_loss: 22.3952326742 - val_loss: 179.8682699764 - val_recon_loss: 168.9863083783 - val_kl_loss: 21.7639270109\n",
      "Saving best state of network...\n",
      "Best State was in Epoch 398\n",
      "Duration of model training in run 4: 0 hours, 17 minutes and 55 seconds.\n",
      "\n",
      "INITIALIZING NEW NETWORK..............\n",
      "Encoder Architecture:\n",
      "\tInput Layer in, out and cond: 4000 256 1\n",
      "\tHidden Layer 1 in/out: 256 256\n",
      "\tHidden Layer 2 in/out: 256 256\n",
      "\tMean/Var Layer in/out: 256 1519\n",
      "Decoder Architecture:\n",
      "\tMasked linear layer in, ext_m, ext, cond, out:  1519 0 0 1 4000\n",
      "\twith hard mask.\n",
      "Last Decoder layer: softmax\n",
      "Preparing (20885, 4000)\n",
      "Instantiating dataset\n",
      "Init the group lasso proximal operator for the main terms.\n",
      " |█████████████-------| 69.2%  - epoch_loss: 245.2661675693 - epoch_recon_loss: 233.8614457319 - epoch_kl_loss: 22.8094428315 - val_loss: 181.0306656781 - val_recon_loss: 170.1771724925 - val_kl_loss: 21.70698446399656\n",
      "ADJUSTED LR\n",
      " |██████████████------| 72.5%  - epoch_loss: 243.3098857646 - epoch_recon_loss: 231.8760778726 - epoch_kl_loss: 22.8676161215 - val_loss: 180.7070204791 - val_recon_loss: 169.7901979334 - val_kl_loss: 21.8336443060\n",
      "ADJUSTED LR\n",
      " |████████████████----| 80.2%  - epoch_loss: 242.6929177005 - epoch_recon_loss: 231.2201991698 - epoch_kl_loss: 22.9454373859 - val_loss: 180.5772902545 - val_recon_loss: 169.7087016386 - val_kl_loss: 21.7371751000\n",
      "ADJUSTED LR\n",
      " |████████████████----| 83.5%  - epoch_loss: 243.8646401126 - epoch_recon_loss: 232.4206239869 - epoch_kl_loss: 22.8880325759 - val_loss: 181.1567975213 - val_recon_loss: 170.2739015467 - val_kl_loss: 21.7657919491\n",
      "ADJUSTED LR\n",
      " |█████████████████---| 86.8%  - epoch_loss: 242.0210125982 - epoch_recon_loss: 230.5990868393 - epoch_kl_loss: 22.8438516863 - val_loss: 180.7624852798 - val_recon_loss: 169.8507887896 - val_kl_loss: 21.8233947754\n",
      "ADJUSTED LR\n",
      " |█████████████████---| 89.5%  - epoch_loss: 243.0889488791 - epoch_recon_loss: 231.6821927440 - epoch_kl_loss: 22.8135125945 - val_loss: 180.6617682962 - val_recon_loss: 169.7691381118 - val_kl_loss: 21.7852607054\n",
      "Stopping early: no improvement of more than 0 nats in 50 epochs\n",
      "If the early stopping criterion is too strong, please instantiate it with different parameters in the train method.\n",
      "Saving best state of network...\n",
      "Best State was in Epoch 306\n",
      "Duration of model training in run 5: 0 hours, 16 minutes and 0 seconds.\n",
      "\n",
      "INITIALIZING NEW NETWORK..............\n",
      "Encoder Architecture:\n",
      "\tInput Layer in, out and cond: 4000 256 1\n",
      "\tHidden Layer 1 in/out: 256 256\n",
      "\tHidden Layer 2 in/out: 256 256\n",
      "\tMean/Var Layer in/out: 256 1519\n",
      "Decoder Architecture:\n",
      "\tMasked linear layer in, ext_m, ext, cond, out:  1519 0 0 1 4000\n",
      "\twith hard mask.\n",
      "Last Decoder layer: softmax\n",
      "Preparing (20885, 4000)\n",
      "Instantiating dataset\n",
      "Init the group lasso proximal operator for the main terms.\n",
      " |██████████████------| 72.2%  - epoch_loss: 243.3461990875 - epoch_recon_loss: 232.0281163430 - epoch_kl_loss: 22.6361661639 - val_loss: 180.9254006778 - val_recon_loss: 170.1562069164 - val_kl_loss: 21.53838898157844\n",
      "ADJUSTED LR\n",
      " |████████████████----| 80.5%  - epoch_loss: 244.5444396142 - epoch_recon_loss: 233.2359618103 - epoch_kl_loss: 22.6169540250 - val_loss: 180.3126696418 - val_recon_loss: 169.3361762552 - val_kl_loss: 21.9529821732\n",
      "ADJUSTED LR\n",
      " |████████████████----| 84.8%  - epoch_loss: 243.3900195271 - epoch_recon_loss: 232.0901633542 - epoch_kl_loss: 22.5997118139 - val_loss: 180.1482570873 - val_recon_loss: 169.1839617561 - val_kl_loss: 21.9285907745\n",
      "ADJUSTED LR\n",
      " |█████████████████---| 89.8%  - epoch_loss: 244.4912706232 - epoch_recon_loss: 233.1325302643 - epoch_kl_loss: 22.7174798745 - val_loss: 179.7861175537 - val_recon_loss: 168.8128823673 - val_kl_loss: 21.9464714948\n",
      "ADJUSTED LR\n",
      " |██████████████████--| 93.0%  - epoch_loss: 243.1433194738 - epoch_recon_loss: 231.8613386089 - epoch_kl_loss: 22.5639618465 - val_loss: 180.3496775908 - val_recon_loss: 169.3643852683 - val_kl_loss: 21.9705831864\n",
      "ADJUSTED LR\n",
      " |███████████████████-| 96.2%  - epoch_loss: 243.5912197398 - epoch_recon_loss: 232.2314211268 - epoch_kl_loss: 22.7195973558 - val_loss: 179.4555987190 - val_recon_loss: 168.4697445140 - val_kl_loss: 21.9717120002\n",
      "ADJUSTED LR\n",
      " |███████████████████-| 99.0%  - epoch_loss: 244.0686178402 - epoch_recon_loss: 232.7530518616 - epoch_kl_loss: 22.6311324113 - val_loss: 179.9696323170 - val_recon_loss: 168.9820951574 - val_kl_loss: 21.9750694948\n",
      "Stopping early: no improvement of more than 0 nats in 50 epochs\n",
      "If the early stopping criterion is too strong, please instantiate it with different parameters in the train method.\n",
      "Saving best state of network...\n",
      "Best State was in Epoch 344\n",
      "Duration of model training in run 6: 0 hours, 17 minutes and 44 seconds.\n",
      "\n",
      "INITIALIZING NEW NETWORK..............\n",
      "Encoder Architecture:\n",
      "\tInput Layer in, out and cond: 4000 256 1\n",
      "\tHidden Layer 1 in/out: 256 256\n",
      "\tHidden Layer 2 in/out: 256 256\n",
      "\tMean/Var Layer in/out: 256 1519\n",
      "Decoder Architecture:\n",
      "\tMasked linear layer in, ext_m, ext, cond, out:  1519 0 0 1 4000\n",
      "\twith hard mask.\n",
      "Last Decoder layer: softmax\n",
      "Preparing (20885, 4000)\n",
      "Instantiating dataset\n",
      "Init the group lasso proximal operator for the main terms.\n",
      " |██████████████████--| 90.0%  - epoch_loss: 243.7078647743 - epoch_recon_loss: 232.6290750309 - epoch_kl_loss: 22.1575797075 - val_loss: 178.3889519187 - val_recon_loss: 167.4645663991 - val_kl_loss: 21.84877339530513\n",
      "ADJUSTED LR\n",
      " |████████████████████| 100.0%  - epoch_loss: 242.8864622570 - epoch_recon_loss: 231.8497988123 - epoch_kl_loss: 22.0733272008 - val_loss: 178.2484005199 - val_recon_loss: 167.4837700339 - val_kl_loss: 21.5292587280\n",
      "\n",
      "ADJUSTED LR\n",
      "Saving best state of network...\n",
      "Best State was in Epoch 385\n",
      "Duration of model training in run 7: 0 hours, 17 minutes and 57 seconds.\n",
      "\n",
      "INITIALIZING NEW NETWORK..............\n",
      "Encoder Architecture:\n",
      "\tInput Layer in, out and cond: 4000 256 1\n",
      "\tHidden Layer 1 in/out: 256 256\n",
      "\tHidden Layer 2 in/out: 256 256\n",
      "\tMean/Var Layer in/out: 256 1519\n",
      "Decoder Architecture:\n",
      "\tMasked linear layer in, ext_m, ext, cond, out:  1519 0 0 1 4000\n",
      "\twith hard mask.\n",
      "Last Decoder layer: softmax\n",
      "Preparing (20885, 4000)\n",
      "Instantiating dataset\n",
      "Init the group lasso proximal operator for the main terms.\n",
      " |███████████████████-| 98.8%  - epoch_loss: 243.3135561781 - epoch_recon_loss: 231.9958175348 - epoch_kl_loss: 22.6354765341 - val_loss: 177.8710461785 - val_recon_loss: 166.9140490364 - val_kl_loss: 21.91399159152135\n",
      "ADJUSTED LR\n",
      " |████████████████████| 100.0%  - epoch_loss: 243.7822911269 - epoch_recon_loss: 232.4692698368 - epoch_kl_loss: 22.6260433586 - val_loss: 177.3209910673 - val_recon_loss: 166.2698086009 - val_kl_loss: 22.1023675133\n",
      "Saving best state of network...\n",
      "Best State was in Epoch 399\n",
      "Duration of model training in run 8: 0 hours, 17 minutes and 56 seconds.\n",
      "\n",
      "INITIALIZING NEW NETWORK..............\n",
      "Encoder Architecture:\n",
      "\tInput Layer in, out and cond: 4000 256 1\n",
      "\tHidden Layer 1 in/out: 256 256\n",
      "\tHidden Layer 2 in/out: 256 256\n",
      "\tMean/Var Layer in/out: 256 1519\n",
      "Decoder Architecture:\n",
      "\tMasked linear layer in, ext_m, ext, cond, out:  1519 0 0 1 4000\n",
      "\twith hard mask.\n",
      "Last Decoder layer: softmax\n",
      "Preparing (10447, 4000)\n",
      "Instantiating dataset\n",
      "Init the group lasso proximal operator for the main terms.\n",
      " |███████-------------| 38.0%  - epoch_loss: 245.1548765028 - epoch_recon_loss: 233.2902260858 - epoch_kl_loss: 23.7292987463 - val_loss: 194.1361050076 - val_recon_loss: 183.4880727132 - val_kl_loss: 21.2960622576727\n",
      "ADJUSTED LR\n",
      " |████████------------| 44.0%  - epoch_loss: 245.2681911572 - epoch_recon_loss: 233.3375939034 - epoch_kl_loss: 23.8611946364 - val_loss: 194.4341379801 - val_recon_loss: 183.6387583415 - val_kl_loss: 21.5907594893\n",
      "ADJUSTED LR\n",
      " |█████████-----------| 47.2%  - epoch_loss: 245.3010179674 - epoch_recon_loss: 233.4131733662 - epoch_kl_loss: 23.7756880425 - val_loss: 196.0863054064 - val_recon_loss: 185.1612260607 - val_kl_loss: 21.8501605988\n",
      "ADJUSTED LR\n",
      " |██████████----------| 50.5%  - epoch_loss: 243.9633263253 - epoch_recon_loss: 232.1250408276 - epoch_kl_loss: 23.6765724646 - val_loss: 197.3925052219 - val_recon_loss: 186.5222337511 - val_kl_loss: 21.7405393389\n",
      "ADJUSTED LR\n",
      " |██████████----------| 53.2%  - epoch_loss: 245.7192504470 - epoch_recon_loss: 233.8171496005 - epoch_kl_loss: 23.8042001724 - val_loss: 195.8281877306 - val_recon_loss: 184.9363674588 - val_kl_loss: 21.7836377886\n",
      "Stopping early: no improvement of more than 0 nats in 50 epochs\n",
      "If the early stopping criterion is too strong, please instantiate it with different parameters in the train method.\n",
      "Saving best state of network...\n",
      "Best State was in Epoch 161\n",
      "Duration of model training in run 1: 0 hours, 4 minutes and 48 seconds.\n",
      "\n",
      "INITIALIZING NEW NETWORK..............\n",
      "Encoder Architecture:\n",
      "\tInput Layer in, out and cond: 4000 256 1\n",
      "\tHidden Layer 1 in/out: 256 256\n",
      "\tHidden Layer 2 in/out: 256 256\n",
      "\tMean/Var Layer in/out: 256 1519\n",
      "Decoder Architecture:\n",
      "\tMasked linear layer in, ext_m, ext, cond, out:  1519 0 0 1 4000\n",
      "\twith hard mask.\n",
      "Last Decoder layer: softmax\n",
      "Preparing (10447, 4000)\n",
      "Instantiating dataset\n",
      "Init the group lasso proximal operator for the main terms.\n",
      " |██████--------------| 31.8%  - epoch_loss: 245.1418188971 - epoch_recon_loss: 233.2629196579 - epoch_kl_loss: 23.7577970608 - val_loss: 200.1157565647 - val_recon_loss: 189.1219567193 - val_kl_loss: 21.9876011742993\n",
      "ADJUSTED LR\n",
      " |█████████-----------| 46.5%  - epoch_loss: 245.6487255612 - epoch_recon_loss: 233.6132499076 - epoch_kl_loss: 24.0709512298 - val_loss: 195.3569166395 - val_recon_loss: 184.5931888156 - val_kl_loss: 21.5274586148\n",
      "ADJUSTED LR\n",
      " |█████████-----------| 49.8%  - epoch_loss: 245.8230075321 - epoch_recon_loss: 233.7789521088 - epoch_kl_loss: 24.0881112073 - val_loss: 198.0487942166 - val_recon_loss: 187.0354342990 - val_kl_loss: 22.0267132653\n",
      "ADJUSTED LR\n",
      " |██████████----------| 53.0%  - epoch_loss: 245.0597513560 - epoch_recon_loss: 233.0350273751 - epoch_kl_loss: 24.0494462348 - val_loss: 198.2041541206 - val_recon_loss: 187.3235999213 - val_kl_loss: 21.7611098819\n",
      "ADJUSTED LR\n",
      " |███████████---------| 55.8%  - epoch_loss: 245.4280651196 - epoch_recon_loss: 233.4163527618 - epoch_kl_loss: 24.0234278086 - val_loss: 200.3247612847 - val_recon_loss: 189.2900950114 - val_kl_loss: 22.0693261888\n",
      "Stopping early: no improvement of more than 0 nats in 50 epochs\n",
      "If the early stopping criterion is too strong, please instantiate it with different parameters in the train method.\n",
      "Saving best state of network...\n",
      "Best State was in Epoch 171\n",
      "Duration of model training in run 2: 0 hours, 5 minutes and 2 seconds.\n",
      "\n",
      "INITIALIZING NEW NETWORK..............\n",
      "Encoder Architecture:\n",
      "\tInput Layer in, out and cond: 4000 256 1\n",
      "\tHidden Layer 1 in/out: 256 256\n",
      "\tHidden Layer 2 in/out: 256 256\n",
      "\tMean/Var Layer in/out: 256 1519\n",
      "Decoder Architecture:\n",
      "\tMasked linear layer in, ext_m, ext, cond, out:  1519 0 0 1 4000\n",
      "\twith hard mask.\n",
      "Last Decoder layer: softmax\n",
      "Preparing (10447, 4000)\n",
      "Instantiating dataset\n",
      "Init the group lasso proximal operator for the main terms.\n",
      " |███████-------------| 37.2%  - epoch_loss: 243.9502821226 - epoch_recon_loss: 231.9286323754 - epoch_kl_loss: 24.0432981027 - val_loss: 196.0818345812 - val_recon_loss: 185.1043124729 - val_kl_loss: 21.9550457001185\n",
      "ADJUSTED LR\n",
      " |████████------------| 44.5%  - epoch_loss: 243.5080535476 - epoch_recon_loss: 231.4433375178 - epoch_kl_loss: 24.1294328586 - val_loss: 191.5665690104 - val_recon_loss: 180.7518836127 - val_kl_loss: 21.6293707954\n",
      "ADJUSTED LR\n",
      " |█████████-----------| 48.0%  - epoch_loss: 244.1461981696 - epoch_recon_loss: 232.0634188265 - epoch_kl_loss: 24.1655578356 - val_loss: 194.6416202121 - val_recon_loss: 183.7831878662 - val_kl_loss: 21.7168638441\n",
      "ADJUSTED LR\n",
      " |██████████----------| 51.2%  - epoch_loss: 242.7418433524 - epoch_recon_loss: 230.6798468925 - epoch_kl_loss: 24.1239924044 - val_loss: 193.7447594537 - val_recon_loss: 182.8375379774 - val_kl_loss: 21.8144372304\n",
      "ADJUSTED LR\n",
      " |██████████----------| 54.5%  - epoch_loss: 243.6525841790 - epoch_recon_loss: 231.5443348240 - epoch_kl_loss: 24.2164987873 - val_loss: 195.3792741564 - val_recon_loss: 184.4153340658 - val_kl_loss: 21.9278835720\n",
      "ADJUSTED LR\n",
      " |███████████---------| 57.2%  - epoch_loss: 245.6046416824 - epoch_recon_loss: 233.5909252682 - epoch_kl_loss: 24.0274309725 - val_loss: 195.0782267253 - val_recon_loss: 184.1811014811 - val_kl_loss: 21.7942500644\n",
      "Stopping early: no improvement of more than 0 nats in 50 epochs\n",
      "If the early stopping criterion is too strong, please instantiate it with different parameters in the train method.\n",
      "Saving best state of network...\n",
      "Best State was in Epoch 177\n",
      "Duration of model training in run 3: 0 hours, 5 minutes and 7 seconds.\n",
      "\n",
      "INITIALIZING NEW NETWORK..............\n",
      "Encoder Architecture:\n",
      "\tInput Layer in, out and cond: 4000 256 1\n",
      "\tHidden Layer 1 in/out: 256 256\n",
      "\tHidden Layer 2 in/out: 256 256\n",
      "\tMean/Var Layer in/out: 256 1519\n",
      "Decoder Architecture:\n",
      "\tMasked linear layer in, ext_m, ext, cond, out:  1519 0 0 1 4000\n",
      "\twith hard mask.\n",
      "Last Decoder layer: softmax\n",
      "Preparing (10447, 4000)\n",
      "Instantiating dataset\n",
      "Init the group lasso proximal operator for the main terms.\n",
      " |█████████-----------| 47.8%  - epoch_loss: 243.3683976869 - epoch_recon_loss: 231.1095183089 - epoch_kl_loss: 24.5177592716 - val_loss: 190.8737114800 - val_recon_loss: 179.7580108643 - val_kl_loss: 22.2314037747358\n",
      "ADJUSTED LR\n",
      " |██████████----------| 51.0%  - epoch_loss: 243.9245081721 - epoch_recon_loss: 231.6755294800 - epoch_kl_loss: 24.4979564821 - val_loss: 189.2887725830 - val_recon_loss: 178.1321156820 - val_kl_loss: 22.3133178287\n",
      "ADJUSTED LR\n",
      " |██████████----------| 54.5%  - epoch_loss: 243.1608369157 - epoch_recon_loss: 230.9242211419 - epoch_kl_loss: 24.4732308259 - val_loss: 192.6610938178 - val_recon_loss: 181.4650556776 - val_kl_loss: 22.3920783997\n",
      "ADJUSTED LR\n",
      " |███████████---------| 57.8%  - epoch_loss: 243.6244442914 - epoch_recon_loss: 231.3429105604 - epoch_kl_loss: 24.5630656577 - val_loss: 193.1790517171 - val_recon_loss: 181.9576568604 - val_kl_loss: 22.4427926805\n",
      "ADJUSTED LR\n",
      " |████████████--------| 61.8%  - epoch_loss: 242.1570024233 - epoch_recon_loss: 229.8761120358 - epoch_kl_loss: 24.5617803625 - val_loss: 189.3012915717 - val_recon_loss: 178.0867140028 - val_kl_loss: 22.4291576809\n",
      "ADJUSTED LR\n",
      " |█████████████-------| 65.0%  - epoch_loss: 243.5423035493 - epoch_recon_loss: 231.3437788680 - epoch_kl_loss: 24.3970489502 - val_loss: 192.8468068441 - val_recon_loss: 181.6246490479 - val_kl_loss: 22.4443149567\n",
      "ADJUSTED LR\n",
      " |█████████████-------| 68.2%  - epoch_loss: 243.5807521923 - epoch_recon_loss: 231.3241383321 - epoch_kl_loss: 24.5132267153 - val_loss: 192.1063503689 - val_recon_loss: 180.9101630317 - val_kl_loss: 22.3923738268\n",
      "ADJUSTED LR\n",
      " |██████████████------| 71.0%  - epoch_loss: 244.8337688962 - epoch_recon_loss: 232.5531785295 - epoch_kl_loss: 24.5611800116 - val_loss: 192.0253312853 - val_recon_loss: 181.0168067084 - val_kl_loss: 22.0170544518\n",
      "Stopping early: no improvement of more than 0 nats in 50 epochs\n",
      "If the early stopping criterion is too strong, please instantiate it with different parameters in the train method.\n",
      "Saving best state of network...\n",
      "Best State was in Epoch 232\n",
      "Duration of model training in run 4: 0 hours, 6 minutes and 21 seconds.\n",
      "\n",
      "INITIALIZING NEW NETWORK..............\n",
      "Encoder Architecture:\n",
      "\tInput Layer in, out and cond: 4000 256 1\n",
      "\tHidden Layer 1 in/out: 256 256\n",
      "\tHidden Layer 2 in/out: 256 256\n",
      "\tMean/Var Layer in/out: 256 1519\n",
      "Decoder Architecture:\n",
      "\tMasked linear layer in, ext_m, ext, cond, out:  1519 0 0 1 4000\n",
      "\twith hard mask.\n",
      "Last Decoder layer: softmax\n",
      "Preparing (10447, 4000)\n",
      "Instantiating dataset\n",
      "Init the group lasso proximal operator for the main terms.\n",
      " |██████████----------| 54.8%  - epoch_loss: 242.7457440351 - epoch_recon_loss: 230.8584666897 - epoch_kl_loss: 23.7745569332 - val_loss: 190.9052039252 - val_recon_loss: 180.1730228000 - val_kl_loss: 21.4643614027938\n",
      "ADJUSTED LR\n",
      " |███████████---------| 58.0%  - epoch_loss: 242.6917308086 - epoch_recon_loss: 230.7860251762 - epoch_kl_loss: 23.8114115999 - val_loss: 187.7687700060 - val_recon_loss: 176.8612077501 - val_kl_loss: 21.8151251475\n",
      "ADJUSTED LR\n",
      " |████████████--------| 61.3%  - epoch_loss: 242.4240722656 - epoch_recon_loss: 230.5713109197 - epoch_kl_loss: 23.7055241611 - val_loss: 189.5359208849 - val_recon_loss: 178.6222941081 - val_kl_loss: 21.8272488912\n",
      "ADJUSTED LR\n",
      " |████████████--------| 64.0%  - epoch_loss: 240.9856111681 - epoch_recon_loss: 229.0663819184 - epoch_kl_loss: 23.8384589376 - val_loss: 187.3205600315 - val_recon_loss: 176.5418395996 - val_kl_loss: 21.5574357775\n",
      "Stopping early: no improvement of more than 0 nats in 50 epochs\n",
      "If the early stopping criterion is too strong, please instantiate it with different parameters in the train method.\n",
      "Saving best state of network...\n",
      "Best State was in Epoch 204\n",
      "Duration of model training in run 5: 0 hours, 5 minutes and 44 seconds.\n",
      "\n",
      "INITIALIZING NEW NETWORK..............\n",
      "Encoder Architecture:\n",
      "\tInput Layer in, out and cond: 4000 256 1\n",
      "\tHidden Layer 1 in/out: 256 256\n",
      "\tHidden Layer 2 in/out: 256 256\n",
      "\tMean/Var Layer in/out: 256 1519\n",
      "Decoder Architecture:\n",
      "\tMasked linear layer in, ext_m, ext, cond, out:  1519 0 0 1 4000\n",
      "\twith hard mask.\n",
      "Last Decoder layer: softmax\n",
      "Preparing (10447, 4000)\n",
      "Instantiating dataset\n",
      "Init the group lasso proximal operator for the main terms.\n",
      " |██████████----------| 51.0%  - epoch_loss: 242.4646068264 - epoch_recon_loss: 230.5836429080 - epoch_kl_loss: 23.7619282748 - val_loss: 191.2967563205 - val_recon_loss: 180.4931420220 - val_kl_loss: 21.6072317759462\n",
      "ADJUSTED LR\n",
      " |███████████---------| 58.8%  - epoch_loss: 242.3448626544 - epoch_recon_loss: 230.3685636778 - epoch_kl_loss: 23.9525975408 - val_loss: 191.4885219998 - val_recon_loss: 180.5039418538 - val_kl_loss: 21.9691535102\n",
      "ADJUSTED LR\n",
      " |████████████--------| 62.0%  - epoch_loss: 242.9107072160 - epoch_recon_loss: 230.9874558320 - epoch_kl_loss: 23.8465036444 - val_loss: 187.9840732151 - val_recon_loss: 177.1696675618 - val_kl_loss: 21.6288093991\n",
      "ADJUSTED LR\n",
      " |█████████████-------| 65.2%  - epoch_loss: 243.2208068435 - epoch_recon_loss: 231.3462130572 - epoch_kl_loss: 23.7491867478 - val_loss: 190.9483439128 - val_recon_loss: 180.1383158366 - val_kl_loss: 21.6200553046\n",
      "ADJUSTED LR\n",
      " |█████████████-------| 68.0%  - epoch_loss: 240.6630030452 - epoch_recon_loss: 228.7840491630 - epoch_kl_loss: 23.7579073004 - val_loss: 190.3336859809 - val_recon_loss: 179.5448337131 - val_kl_loss: 21.5777113173\n",
      "Stopping early: no improvement of more than 0 nats in 50 epochs\n",
      "If the early stopping criterion is too strong, please instantiate it with different parameters in the train method.\n",
      "Saving best state of network...\n",
      "Best State was in Epoch 220\n",
      "Duration of model training in run 6: 0 hours, 6 minutes and 7 seconds.\n",
      "\n",
      "INITIALIZING NEW NETWORK..............\n",
      "Encoder Architecture:\n",
      "\tInput Layer in, out and cond: 4000 256 1\n",
      "\tHidden Layer 1 in/out: 256 256\n",
      "\tHidden Layer 2 in/out: 256 256\n",
      "\tMean/Var Layer in/out: 256 1519\n",
      "Decoder Architecture:\n",
      "\tMasked linear layer in, ext_m, ext, cond, out:  1519 0 0 1 4000\n",
      "\twith hard mask.\n",
      "Last Decoder layer: softmax\n",
      "Preparing (10447, 4000)\n",
      "Instantiating dataset\n",
      "Init the group lasso proximal operator for the main terms.\n",
      " |██████████----------| 54.2%  - epoch_loss: 242.3233440502 - epoch_recon_loss: 230.6593780518 - epoch_kl_loss: 23.3279319196 - val_loss: 189.4680175781 - val_recon_loss: 178.6381852892 - val_kl_loss: 21.6596647898678\n",
      "ADJUSTED LR\n",
      " |███████████---------| 59.0%  - epoch_loss: 242.0672900226 - epoch_recon_loss: 230.2785881661 - epoch_kl_loss: 23.5774054656 - val_loss: 188.6480865479 - val_recon_loss: 177.8547007243 - val_kl_loss: 21.5867701636\n",
      "ADJUSTED LR\n",
      " |████████████--------| 62.3%  - epoch_loss: 241.6454024444 - epoch_recon_loss: 229.8333088643 - epoch_kl_loss: 23.6241865416 - val_loss: 187.7229444716 - val_recon_loss: 176.9214935303 - val_kl_loss: 21.6029027303\n",
      "ADJUSTED LR\n",
      " |█████████████-------| 67.8%  - epoch_loss: 242.0690392159 - epoch_recon_loss: 230.2454831922 - epoch_kl_loss: 23.6471148568 - val_loss: 187.5584682888 - val_recon_loss: 176.7223900689 - val_kl_loss: 21.6721560160\n",
      "ADJUSTED LR\n",
      " |██████████████------| 71.0%  - epoch_loss: 241.7148816908 - epoch_recon_loss: 229.9225863895 - epoch_kl_loss: 23.5845918398 - val_loss: 188.9256880018 - val_recon_loss: 178.0295054118 - val_kl_loss: 21.7923607296\n",
      "ADJUSTED LR\n",
      " |██████████████------| 74.2%  - epoch_loss: 242.5436345693 - epoch_recon_loss: 230.7681750736 - epoch_kl_loss: 23.5509193266 - val_loss: 188.2182142470 - val_recon_loss: 177.3724551731 - val_kl_loss: 21.6915185716\n",
      "ADJUSTED LR\n",
      " |███████████████-----| 78.5%  - epoch_loss: 242.7036829768 - epoch_recon_loss: 230.9642296868 - epoch_kl_loss: 23.4789084099 - val_loss: 188.3039584690 - val_recon_loss: 177.5048014323 - val_kl_loss: 21.5983113183\n",
      "ADJUSTED LR\n",
      " |████████████████----| 81.8%  - epoch_loss: 242.4553317508 - epoch_recon_loss: 230.7108659487 - epoch_kl_loss: 23.4889302640 - val_loss: 188.0904930962 - val_recon_loss: 177.2701873779 - val_kl_loss: 21.6406101651\n",
      "ADJUSTED LR\n",
      " |█████████████████---| 85.0%  - epoch_loss: 244.1078105617 - epoch_recon_loss: 232.2833775701 - epoch_kl_loss: 23.6488649781 - val_loss: 187.5956658257 - val_recon_loss: 176.7221849230 - val_kl_loss: 21.7469628652\n",
      "ADJUSTED LR\n",
      " |█████████████████---| 87.8%  - epoch_loss: 242.5502391506 - epoch_recon_loss: 230.7435908962 - epoch_kl_loss: 23.6132951685 - val_loss: 189.4690890842 - val_recon_loss: 178.7178683811 - val_kl_loss: 21.5024414062\n",
      "Stopping early: no improvement of more than 0 nats in 50 epochs\n",
      "If the early stopping criterion is too strong, please instantiate it with different parameters in the train method.\n",
      "Saving best state of network...\n",
      "Best State was in Epoch 299\n",
      "Duration of model training in run 7: 0 hours, 7 minutes and 54 seconds.\n",
      "\n",
      "INITIALIZING NEW NETWORK..............\n",
      "Encoder Architecture:\n",
      "\tInput Layer in, out and cond: 4000 256 1\n",
      "\tHidden Layer 1 in/out: 256 256\n",
      "\tHidden Layer 2 in/out: 256 256\n",
      "\tMean/Var Layer in/out: 256 1519\n",
      "Decoder Architecture:\n",
      "\tMasked linear layer in, ext_m, ext, cond, out:  1519 0 0 1 4000\n",
      "\twith hard mask.\n",
      "Last Decoder layer: softmax\n",
      "Preparing (10447, 4000)\n",
      "Instantiating dataset\n",
      "Init the group lasso proximal operator for the main terms.\n",
      " |███████-------------| 35.5%  - epoch_loss: 244.0710630675 - epoch_recon_loss: 232.1609732138 - epoch_kl_loss: 23.8201789598 - val_loss: 198.5812377930 - val_recon_loss: 187.3760596381 - val_kl_loss: 22.4103628794492\n",
      "ADJUSTED LR\n",
      " |████████------------| 40.8%  - epoch_loss: 245.4652246011 - epoch_recon_loss: 233.4695657266 - epoch_kl_loss: 23.9913194244 - val_loss: 196.2053256565 - val_recon_loss: 185.2181447347 - val_kl_loss: 21.9743622674\n",
      "ADJUSTED LR\n",
      " |████████------------| 44.0%  - epoch_loss: 245.0319182937 - epoch_recon_loss: 233.0405122912 - epoch_kl_loss: 23.9828141960 - val_loss: 194.6638607449 - val_recon_loss: 183.6305881076 - val_kl_loss: 22.0665482415\n",
      "ADJUSTED LR\n",
      " |█████████-----------| 47.8%  - epoch_loss: 244.9078703184 - epoch_recon_loss: 232.9457846977 - epoch_kl_loss: 23.9241710869 - val_loss: 194.1584116618 - val_recon_loss: 183.1022169325 - val_kl_loss: 22.1123873393\n",
      "ADJUSTED LR\n",
      " |██████████----------| 51.0%  - epoch_loss: 245.8912866953 - epoch_recon_loss: 233.9085155178 - epoch_kl_loss: 23.9655427417 - val_loss: 194.4935201009 - val_recon_loss: 183.3442738851 - val_kl_loss: 22.2984915839\n",
      "ADJUSTED LR\n",
      " |██████████----------| 54.2%  - epoch_loss: 246.6914705844 - epoch_recon_loss: 234.6951889863 - epoch_kl_loss: 23.9925627322 - val_loss: 194.8099602593 - val_recon_loss: 183.9277225071 - val_kl_loss: 21.7644706302\n",
      "ADJUSTED LR\n",
      " |███████████---------| 57.0%  - epoch_loss: 244.7418396408 - epoch_recon_loss: 232.7823321368 - epoch_kl_loss: 23.9190157555 - val_loss: 194.9523095025 - val_recon_loss: 183.9716610379 - val_kl_loss: 21.9612971412\n",
      "Stopping early: no improvement of more than 0 nats in 50 epochs\n",
      "If the early stopping criterion is too strong, please instantiate it with different parameters in the train method.\n",
      "Saving best state of network...\n",
      "Best State was in Epoch 176\n",
      "Duration of model training in run 8: 0 hours, 5 minutes and 6 seconds.\n",
      "\n",
      "INITIALIZING NEW NETWORK..............\n",
      "Encoder Architecture:\n",
      "\tInput Layer in, out and cond: 4000 256 1\n",
      "\tHidden Layer 1 in/out: 256 256\n",
      "\tHidden Layer 2 in/out: 256 256\n",
      "\tMean/Var Layer in/out: 256 1519\n",
      "Decoder Architecture:\n",
      "\tMasked linear layer in, ext_m, ext, cond, out:  1519 0 0 1 4000\n",
      "\twith hard mask.\n",
      "Last Decoder layer: softmax\n",
      "Preparing (4200, 4000)\n",
      "Instantiating dataset\n",
      "Init the group lasso proximal operator for the main terms.\n",
      " |████████------------| 44.2%  - epoch_loss: 249.9669835409 - epoch_recon_loss: 239.8145161947 - epoch_kl_loss: 20.3049338659 - val_loss: 210.6583290100 - val_recon_loss: 202.4339714050 - val_kl_loss: 16.4487135410632\n",
      "ADJUSTED LR\n",
      " |█████████-----------| 47.5%  - epoch_loss: 253.5098108927 - epoch_recon_loss: 243.4233439128 - epoch_kl_loss: 20.1729288101 - val_loss: 213.5651359558 - val_recon_loss: 205.1587486267 - val_kl_loss: 16.8127746582\n",
      "ADJUSTED LR\n",
      " |██████████----------| 53.2%  - epoch_loss: 252.8967809041 - epoch_recon_loss: 242.8773849487 - epoch_kl_loss: 20.0387909571 - val_loss: 211.9504814148 - val_recon_loss: 203.5619506836 - val_kl_loss: 16.7770600319\n",
      "ADJUSTED LR\n",
      " |███████████---------| 56.5%  - epoch_loss: 250.1206181844 - epoch_recon_loss: 239.9922775269 - epoch_kl_loss: 20.2566834132 - val_loss: 209.9936943054 - val_recon_loss: 201.5378456116 - val_kl_loss: 16.9117021561\n",
      "ADJUSTED LR\n",
      " |████████████--------| 60.8%  - epoch_loss: 252.1010528564 - epoch_recon_loss: 242.0745234172 - epoch_kl_loss: 20.0530588150 - val_loss: 214.6355819702 - val_recon_loss: 206.3396835327 - val_kl_loss: 16.5917992592\n",
      "ADJUSTED LR\n",
      " |████████████--------| 64.0%  - epoch_loss: 248.6248703003 - epoch_recon_loss: 238.5522160848 - epoch_kl_loss: 20.1453106562 - val_loss: 210.8523254395 - val_recon_loss: 202.4466476440 - val_kl_loss: 16.8113598824\n",
      "ADJUSTED LR\n",
      " |██████████████------| 70.5%  - epoch_loss: 252.7185801188 - epoch_recon_loss: 242.5446533203 - epoch_kl_loss: 20.3478492101 - val_loss: 209.4498481750 - val_recon_loss: 201.0221176147 - val_kl_loss: 16.8554615974\n",
      "ADJUSTED LR\n",
      " |██████████████------| 73.8%  - epoch_loss: 249.6450719198 - epoch_recon_loss: 239.4716125488 - epoch_kl_loss: 20.3469158808 - val_loss: 216.0137863159 - val_recon_loss: 207.5537033081 - val_kl_loss: 16.9201712608\n",
      "ADJUSTED LR\n",
      " |███████████████-----| 77.0%  - epoch_loss: 251.5459762573 - epoch_recon_loss: 241.4718917847 - epoch_kl_loss: 20.1481673559 - val_loss: 211.0990524292 - val_recon_loss: 202.7457923889 - val_kl_loss: 16.7065110207\n",
      "ADJUSTED LR\n",
      " |███████████████-----| 79.8%  - epoch_loss: 250.9968058268 - epoch_recon_loss: 240.8329869588 - epoch_kl_loss: 20.3276385625 - val_loss: 214.6994895935 - val_recon_loss: 206.2041893005 - val_kl_loss: 16.9905934334\n",
      "Stopping early: no improvement of more than 0 nats in 50 epochs\n",
      "If the early stopping criterion is too strong, please instantiate it with different parameters in the train method.\n",
      "Saving best state of network...\n",
      "Best State was in Epoch 267\n",
      "Duration of model training in run 1: 0 hours, 2 minutes and 53 seconds.\n",
      "\n",
      "INITIALIZING NEW NETWORK..............\n",
      "Encoder Architecture:\n",
      "\tInput Layer in, out and cond: 4000 256 1\n",
      "\tHidden Layer 1 in/out: 256 256\n",
      "\tHidden Layer 2 in/out: 256 256\n",
      "\tMean/Var Layer in/out: 256 1519\n",
      "Decoder Architecture:\n",
      "\tMasked linear layer in, ext_m, ext, cond, out:  1519 0 0 1 4000\n",
      "\twith hard mask.\n",
      "Last Decoder layer: softmax\n",
      "Preparing (4200, 4000)\n",
      "Instantiating dataset\n",
      "Init the group lasso proximal operator for the main terms.\n",
      " |██████--------------| 31.5%  - epoch_loss: 257.4829188029 - epoch_recon_loss: 245.7977879842 - epoch_kl_loss: 23.3702646255 - val_loss: 220.9172477722 - val_recon_loss: 211.4636116028 - val_kl_loss: 18.9072794914397\n",
      "ADJUSTED LR\n",
      " |███████-------------| 36.8%  - epoch_loss: 260.3630666097 - epoch_recon_loss: 248.8179143270 - epoch_kl_loss: 23.0903052012 - val_loss: 221.2478485107 - val_recon_loss: 211.8958244324 - val_kl_loss: 18.7040467262\n",
      "ADJUSTED LR\n",
      " |████████------------| 40.0%  - epoch_loss: 257.3427576701 - epoch_recon_loss: 245.7278681437 - epoch_kl_loss: 23.2297826767 - val_loss: 219.1041297913 - val_recon_loss: 209.6056213379 - val_kl_loss: 18.9970111847\n",
      "ADJUSTED LR\n",
      " |████████------------| 43.2%  - epoch_loss: 257.9498082479 - epoch_recon_loss: 246.3095921834 - epoch_kl_loss: 23.2804325104 - val_loss: 220.8950653076 - val_recon_loss: 211.3893508911 - val_kl_loss: 19.0114383698\n",
      "ADJUSTED LR\n",
      " |█████████-----------| 46.0%  - epoch_loss: 256.6795471191 - epoch_recon_loss: 245.0691624959 - epoch_kl_loss: 23.2207742055 - val_loss: 218.8064270020 - val_recon_loss: 209.4861984253 - val_kl_loss: 18.6404571533\n",
      "Stopping early: no improvement of more than 0 nats in 50 epochs\n",
      "If the early stopping criterion is too strong, please instantiate it with different parameters in the train method.\n",
      "Saving best state of network...\n",
      "Best State was in Epoch 132\n",
      "Duration of model training in run 2: 0 hours, 1 minutes and 40 seconds.\n",
      "\n",
      "INITIALIZING NEW NETWORK..............\n",
      "Encoder Architecture:\n",
      "\tInput Layer in, out and cond: 4000 256 1\n",
      "\tHidden Layer 1 in/out: 256 256\n",
      "\tHidden Layer 2 in/out: 256 256\n",
      "\tMean/Var Layer in/out: 256 1519\n",
      "Decoder Architecture:\n",
      "\tMasked linear layer in, ext_m, ext, cond, out:  1519 0 0 1 4000\n",
      "\twith hard mask.\n",
      "Last Decoder layer: softmax\n",
      "Preparing (4200, 4000)\n",
      "Instantiating dataset\n",
      "Init the group lasso proximal operator for the main terms.\n",
      " |████████------------| 42.2%  - epoch_loss: 251.1157653809 - epoch_recon_loss: 240.3694646200 - epoch_kl_loss: 21.4925998688 - val_loss: 211.5940208435 - val_recon_loss: 202.4960441589 - val_kl_loss: 18.1959428787040\n",
      "ADJUSTED LR\n",
      " |██████████----------| 52.0%  - epoch_loss: 250.8844980876 - epoch_recon_loss: 240.2684285482 - epoch_kl_loss: 21.2321420670 - val_loss: 212.7420082092 - val_recon_loss: 203.6646881104 - val_kl_loss: 18.1546416283\n",
      "ADJUSTED LR\n",
      " |███████████---------| 55.2%  - epoch_loss: 251.6385894775 - epoch_recon_loss: 240.9353703817 - epoch_kl_loss: 21.4064367930 - val_loss: 211.5267715454 - val_recon_loss: 202.4423866272 - val_kl_loss: 18.1687741280\n",
      "ADJUSTED LR\n",
      " |███████████---------| 58.5%  - epoch_loss: 255.4567652384 - epoch_recon_loss: 244.6867177327 - epoch_kl_loss: 21.5400928497 - val_loss: 213.0127792358 - val_recon_loss: 203.9952621460 - val_kl_loss: 18.0350322723\n",
      "ADJUSTED LR\n",
      " |████████████--------| 61.3%  - epoch_loss: 250.2262903849 - epoch_recon_loss: 239.4697550456 - epoch_kl_loss: 21.5130694071 - val_loss: 212.8043556213 - val_recon_loss: 203.7779312134 - val_kl_loss: 18.0528502464\n",
      "Stopping early: no improvement of more than 0 nats in 50 epochs\n",
      "If the early stopping criterion is too strong, please instantiate it with different parameters in the train method.\n",
      "Saving best state of network...\n",
      "Best State was in Epoch 193\n",
      "Duration of model training in run 3: 0 hours, 2 minutes and 13 seconds.\n"
     ]
    }
   ],
   "source": [
    "for subsample_pct in [50, 25, 10, 5, 1]:\n",
    "    train_expimap_models(dataset=f\"slideseqv2_mouse_hippocampus_subsample_{subsample_pct}pct\",\n",
    "                         cell_type_key=\"cell_type\",\n",
    "                         gp_dict=mouse_combined_new_gp_dict,\n",
    "                         adata_new=None,\n",
    "                         n_start_run=1,\n",
    "                         n_end_run=8,\n",
    "                         n_neighbor_list=[4, 4, 8, 8, 12, 12, 16, 16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861c4cb2-757a-44ca-acf1-b677b8308841",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "toc-showcode": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
