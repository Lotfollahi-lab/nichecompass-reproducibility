{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STACI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Creator**: Sebastian Birk (<sebastian.birk@helmholtz-munich.de>)\n",
    "- **Date of Creation:** 21.11.2023\n",
    "- **Date of Last Modification:** 18.07.2024 (Sebastian Birk; <sebastian.birk@helmholtz-munich.de>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The STACI source code is available at https://github.com/uhlerlab/STACI/blob/master.\n",
    "- The corresponding publication is \"Zhang, X., Wang, X., Shivashankar, G. V. & Uhler, C. Graph-based autoencoder integrates spatial transcriptomics with chromatin images and identifies joint biomarkers for Alzheimerâ€™s disease. Nat. Commun. 13, 7480 (2022)\". Publication at https://www.nature.com/articles/s41467-022-35233-1.\n",
    "- The workflow of this notebook follows the notebook from https://github.com/uhlerlab/STACI/blob/master/train_gae_starmap_multisamples.ipynb."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Run this notebook in the nichecompass-reproducibility environment, installable from ```('../../../envs/environment.yaml')```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../STACI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "import scanpy as sc\n",
    "import squidpy as sq\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "from torch import optim\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import umap\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "import gae.gae.optimizer as optimizer\n",
    "import gae.gae.model\n",
    "import gae.gae.preprocessing as preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Define Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"staci\"\n",
    "latent_key = f\"{model_name}_latent\"\n",
    "mapping_entity_key = \"reference\"\n",
    "condition_key = \"batch\"\n",
    "counts_key = \"counts\"\n",
    "spatial_key = \"spatial\"\n",
    "adj_key = \"spatial_connectivities\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Run Notebook Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.set_figure_params(figsize=(6, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get time of notebook execution for timestamping saved artifacts\n",
    "now = datetime.now()\n",
    "current_timestamp = now.strftime(\"%d%m%Y_%H%M%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Configure Paths and Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st_data_gold_folder_path = \"../../../datasets/st_data/gold\"\n",
    "st_data_results_folder_path = \"../../../datasets/st_data/results\" \n",
    "figure_folder_path = f\"../../../figures\"\n",
    "benchmarking_folder_path = \"../../../artifacts/sample_integration_method_benchmarking\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. STACI Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Define Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_staci_models(dataset,\n",
    "                       reference_batches,\n",
    "                       cell_type_key,\n",
    "                       adata_new=None,\n",
    "                       n_start_run=1,\n",
    "                       n_end_run=8,\n",
    "                       n_neighbor_list=[4, 4, 8, 8, 12, 12, 16, 16],\n",
    "                       filter_genes: bool=False,\n",
    "                       n_svg: int=3000):\n",
    "    \n",
    "    # Settings\n",
    "    use_cuda=True #set to true if GPU is used \n",
    "    fastmode=True #Perform validation during training pass\n",
    "    useSavedMaskedEdges=False #some edges of the adjacency matrices are held-out for validation; set to True to save and use saved version of the edge masks\n",
    "    epochs=1000 #number of training epochs (default was 10000 but after 1000 no improvements achieved anymore)\n",
    "    saveFreq=30 #the model parameters will be saved during training at a frequency defined by this parameter\n",
    "    lr=0.001 #initial learning rate\n",
    "    lr_adv=0.001 #this is ignored if not using an adversarial loss in the latent space (i.e. it is ignored for the default setup of STACI. If a discriminator is trained to use the adversarial loss, this is the learning rate of the discriminator.)\n",
    "    weight_decay=0 #regularization term\n",
    "\n",
    "    dropout=0.01 #neural network dropout term\n",
    "    testNodes=0.1 #fraction of total cells used for testing\n",
    "    valNodes=0.05 #fraction of total cells used for validation\n",
    "    XreconWeight=20  #reconstruction weight of the gene expression\n",
    "    ridgeL=0.01 #regularization weight of the gene dropout parameter\n",
    "    shareGenePi=True #ignored in the default model; This is a parameter to specify how if the gene dropout term is shared for some variants of the ZINB distribution modeling as discussed in the original deep count autoencoder paper.\n",
    "\n",
    "    targetBatch=None #if adversarial loss is used, one possibility is to make all batches look like one target batch. None, if not using this option.\n",
    "    switchFreq=10 #the number of epochs spent on training the model using one sample, before switching to the next sample\n",
    "    name='newModel' #name of the model\n",
    "    \n",
    "    #provide the paths to save the training log, trained models, and plots, and the path to the directory where the data is stored\n",
    "    logsavepath=f'../STACI/log/{dataset}/'+name\n",
    "    modelsavepath=f'../STACI/models/{dataset}/'+name\n",
    "    plotsavepath=f'../STACI/plots/{dataset}/'+name\n",
    "    savedir=f'../STACI/adjacencies/{dataset}/'+name\n",
    "\n",
    "    if not os.path.exists(logsavepath):\n",
    "        os.makedirs(logsavepath)\n",
    "    if not os.path.exists(modelsavepath):\n",
    "        os.makedirs(modelsavepath)\n",
    "    if not os.path.exists(plotsavepath):\n",
    "        os.makedirs(plotsavepath)\n",
    "    if not os.path.exists(savedir):\n",
    "        os.makedirs(savedir)\n",
    "    \n",
    "    # Configure figure folder path\n",
    "    dataset_figure_folder_path = f\"{figure_folder_path}/{dataset}/sample_integration_method_benchmarking/\" \\\n",
    "                                 f\"{model_name}/{current_timestamp}\"\n",
    "    os.makedirs(dataset_figure_folder_path, exist_ok=True)\n",
    "    \n",
    "    # Create new adata to store results from training runs in storage-efficient way\n",
    "    if adata_new is None:\n",
    "        adata_batch_list = []\n",
    "        if reference_batches is not None:\n",
    "            for batch in reference_batches:\n",
    "                adata_batch = ad.read_h5ad(\n",
    "                    f\"{st_data_gold_folder_path}/{dataset}_{batch}.h5ad\")\n",
    "                adata_batch.obs[mapping_entity_key] = \"reference\"\n",
    "                adata_batch_list.append(adata_batch)\n",
    "            adata_original = ad.concat(adata_batch_list, join=\"inner\")\n",
    "        else:\n",
    "            adata_original = ad.read_h5ad(f\"{st_data_gold_folder_path}/{dataset}.h5ad\")\n",
    "\n",
    "        adata_new = sc.AnnData(sp.csr_matrix(\n",
    "            (adata_original.shape[0], adata_original.shape[1]),\n",
    "            dtype=np.float32))\n",
    "        adata_new.var_names = adata_original.var_names\n",
    "        adata_new.obs_names = adata_original.obs_names\n",
    "        adata_new.obs[\"cell_type\"] = adata_original.obs[cell_type_key].values\n",
    "        adata_new.obsm[\"spatial\"] = adata_original.obsm[\"spatial\"]\n",
    "        adata_new.obs[condition_key] = adata_original.obs[condition_key]\n",
    "        adata_new.obs[mapping_entity_key] = adata_original.obs[mapping_entity_key] \n",
    "        del(adata_original)\n",
    "        gc.collect()\n",
    "\n",
    "    model_seeds = list(range(65, 78))\n",
    "    for run_number, n_neighbors in zip(np.arange(n_start_run, n_end_run+1), n_neighbor_list):\n",
    "        # Load data\n",
    "        adata_batch_list = []\n",
    "        if reference_batches is not None:\n",
    "            for batch in reference_batches:\n",
    "                print(f\"Processing batch {batch}...\")\n",
    "                print(\"Loading data...\")\n",
    "                adata_batch = ad.read_h5ad(\n",
    "                    f\"{st_data_gold_folder_path}/{dataset}_{batch}.h5ad\")\n",
    "                adata_batch.obs[mapping_entity_key] = \"reference\"\n",
    "                adata_batch.X = adata_batch.layers[\"counts\"]\n",
    "                # Compute (separate) spatial neighborhood graphs\n",
    "                sq.gr.spatial_neighbors(adata_batch,\n",
    "                                        coord_type=\"generic\",\n",
    "                                        spatial_key=spatial_key,\n",
    "                                        n_neighs=n_neighbors)\n",
    "                # Make adjacency matrix symmetric\n",
    "                adata_batch.obsp[adj_key] = (\n",
    "                    adata_batch.obsp[adj_key].maximum(\n",
    "                        adata_batch.obsp[adj_key].T))\n",
    "                adata_batch_list.append(adata_batch) \n",
    "            adata = ad.concat(adata_batch_list, join=\"inner\")\n",
    "            adata.X = adata.X.toarray()\n",
    "            # Combine spatial neighborhood graphs as disconnected components\n",
    "            batch_connectivities = []\n",
    "            len_before_batch = 0\n",
    "            for i in range(len(adata_batch_list)):\n",
    "                if i == 0: # first batch\n",
    "                    after_batch_connectivities_extension = sp.csr_matrix(\n",
    "                        (adata_batch_list[0].shape[0],\n",
    "                        (adata.shape[0] -\n",
    "                        adata_batch_list[0].shape[0])))\n",
    "                    batch_connectivities.append(sp.hstack(\n",
    "                        (adata_batch_list[0].obsp[adj_key],\n",
    "                        after_batch_connectivities_extension)))\n",
    "                elif i == (len(adata_batch_list) - 1): # last batch\n",
    "                    before_batch_connectivities_extension = sp.csr_matrix(\n",
    "                        (adata_batch_list[i].shape[0],\n",
    "                        (adata.shape[0] -\n",
    "                        adata_batch_list[i].shape[0])))\n",
    "                    batch_connectivities.append(sp.hstack(\n",
    "                        (before_batch_connectivities_extension,\n",
    "                        adata_batch_list[i].obsp[adj_key])))\n",
    "                else: # middle batches\n",
    "                    before_batch_connectivities_extension = sp.csr_matrix(\n",
    "                        (adata_batch_list[i].shape[0], len_before_batch))\n",
    "                    after_batch_connectivities_extension = sp.csr_matrix(\n",
    "                        (adata_batch_list[i].shape[0],\n",
    "                        (adata.shape[0] -\n",
    "                        adata_batch_list[i].shape[0] -\n",
    "                        len_before_batch)))\n",
    "                    batch_connectivities.append(sp.hstack(\n",
    "                        (before_batch_connectivities_extension,\n",
    "                        adata_batch_list[i].obsp[adj_key],\n",
    "                        after_batch_connectivities_extension)))\n",
    "                len_before_batch += adata_batch_list[i].shape[0]\n",
    "            connectivities = sp.vstack(batch_connectivities)\n",
    "            adata.obsp[adj_key] = connectivities\n",
    "            elapsed_time = 0\n",
    "        else:\n",
    "            adata = ad.read_h5ad(f\"{st_data_gold_folder_path}/{dataset}.h5ad\")\n",
    "            # Store raw counts in adata.X\n",
    "            adata.X = adata.layers[\"counts\"]\n",
    "            adata.X = adata.X.toarray()\n",
    "            # Compute (separate) spatial neighborhood graphs\n",
    "            sq.gr.spatial_neighbors(adata,\n",
    "                                    coord_type=\"generic\",\n",
    "                                    spatial_key=spatial_key,\n",
    "                                    n_neighs=n_neighbors)\n",
    "            # Make adjacency matrix symmetric\n",
    "            adata.obsp[adj_key] = (\n",
    "                adata.obsp[adj_key].maximum(\n",
    "                    adata.obsp[adj_key].T))\n",
    "            elapsed_time = 0\n",
    "            \n",
    "        if filter_genes:\n",
    "            sc.pp.filter_genes(adata,\n",
    "                               min_cells=0)\n",
    "            sq.gr.spatial_autocorr(adata, mode=\"moran\", genes=adata.var_names)\n",
    "            sv_genes = adata.uns[\"moranI\"].index[:n_svg].tolist()\n",
    "            adata.var[\"spatially_variable\"] = adata.var_names.isin(sv_genes)\n",
    "            adata = adata[:, adata.var[\"spatially_variable\"] == True]\n",
    "            print(f\"Keeping {len(adata.var_names)} spatially variable genes.\")       \n",
    "        \n",
    "        training_samples = adata.obs[\"batch\"].unique().tolist()\n",
    "        sampleidx = {sample: sample for sample in training_samples}\n",
    "        \n",
    "        maskedgeName = f'knn{n_neighbors}_connectivity'\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        #normalize the gene expression\n",
    "        #batch information should be stored in the metadata as 'sample'\n",
    "        featureslist={}\n",
    "        adj_list = {}\n",
    "        for s in sampleidx.keys():\n",
    "            adata_sample=adata[adata.obs['batch']==sampleidx[s]]\n",
    "            featurelog_train=np.log2(adata_sample.X+1/2)\n",
    "            scaler = MinMaxScaler()\n",
    "            featurelog_train_minmax=np.transpose(scaler.fit_transform(np.transpose(featurelog_train)))\n",
    "            featureslist[s+'X_logminmax']=torch.tensor(featurelog_train_minmax)\n",
    "\n",
    "            adj_list[sampleidx[s]] = adata_sample.obsp[adj_key]\n",
    "            del(adata_sample)\n",
    "            gc.collect()\n",
    "\n",
    "        num_features = adata.shape[1]\n",
    "        \n",
    "        hidden1=3*num_features #Number of units in hidden layer 1\n",
    "        hidden2=3*num_features #Number of units in hidden layer 2\n",
    "        fc_dim1=3*num_features #Number of units in the fully connected layer of the decoder\n",
    "\n",
    "        adjnormlist={}\n",
    "        pos_weightlist={}\n",
    "\n",
    "        normlist={}\n",
    "        for ai in adj_list.keys():\n",
    "            adjnormlist[ai]=preprocessing.preprocess_graph(adj_list[ai])\n",
    "\n",
    "            pos_weightlist[ai] = torch.tensor(float(adj_list[ai].shape[0] * adj_list[ai].shape[0] - adj_list[ai].sum()) / adj_list[ai].sum()) #using full unmasked adj\n",
    "            normlist[ai] = adj_list[ai].shape[0] * adj_list[ai].shape[0] / float((adj_list[ai].shape[0] * adj_list[ai].shape[0] - adj_list[ai].sum()) * 2)\n",
    "            \n",
    "            adj_label=adj_list[ai] + sp.eye(adj_list[ai].shape[0])\n",
    "            adj_list[ai]=torch.tensor(adj_label.todense()) # very memory intensive\n",
    "\n",
    "        rawdata=adata\n",
    "        \n",
    "        features_raw_list={}\n",
    "        for s in sampleidx.keys():\n",
    "            features_raw_list[s+'X_'+'raw']=torch.tensor(rawdata.X[rawdata.obs['batch']==sampleidx[s]])\n",
    "\n",
    "        # Set cuda and seed\n",
    "        np.random.seed(model_seeds[run_number-1])\n",
    "        if use_cuda and (not torch.cuda.is_available()):\n",
    "            print('cuda not available')\n",
    "            use_cuda=False\n",
    "        torch.manual_seed(model_seeds[run_number-1])\n",
    "        if use_cuda:\n",
    "            torch.cuda.manual_seed(model_seeds[run_number-1])\n",
    "            torch.backends.cudnn.enabled = True\n",
    "\n",
    "        # loop over all train/validation sets\n",
    "        np.random.seed(model_seeds[run_number-1])\n",
    "        torch.manual_seed(model_seeds[run_number-1])\n",
    "        if use_cuda:\n",
    "            torch.cuda.manual_seed(model_seeds[run_number-1])\n",
    "            torch.backends.cudnn.enabled = True\n",
    "\n",
    "        mse=torch.nn.MSELoss()\n",
    "        # Create model\n",
    "        model = gae.gae.model.GCNModelVAE_XA_e2_d1_DCA(num_features, hidden1,hidden2,fc_dim1, dropout)\n",
    "        loss_kl=optimizer.optimizer_kl\n",
    "        loss_x=optimizer.optimizer_zinb\n",
    "        loss_a=optimizer.optimizer_CE\n",
    "\n",
    "        if use_cuda:\n",
    "            model.cuda()\n",
    "\n",
    "        optimizerVAEXA = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "        def train(epoch):\n",
    "            t = time.time()\n",
    "            model.train()\n",
    "            optimizerVAEXA.zero_grad()\n",
    "\n",
    "            adj_recon,mu,logvar,z,features_recon = model(features, adj_norm)\n",
    "\n",
    "            loss_kl_train=loss_kl(mu, logvar, train_nodes_idx)\n",
    "            loss_x_train=loss_x(features_recon, features,train_nodes_idx,XreconWeight,ridgeL,features_raw)\n",
    "            loss_a_train=loss_a(adj_recon, adj_label, pos_weight, norm,train_nodes_idx)\n",
    "\n",
    "            loss=loss_kl_train+loss_x_train #for lossXreconOnly_wKL only\n",
    "            loss=loss+loss_a_train\n",
    "\n",
    "            loss.backward()\n",
    "            optimizerVAEXA.step()\n",
    "\n",
    "            if not fastmode:\n",
    "                # Evaluate validation set performance separately,\n",
    "                # deactivates dropout during validation run & no variation in z.\n",
    "                model.eval()\n",
    "                adj_recon,mu,logvar,z, features_recon = model(features, adj_norm)\n",
    "\n",
    "\n",
    "            loss_x_val=loss_x(features_recon, features,val_nodes_idx,XreconWeight,ridgeL,features_raw)\n",
    "            loss_a_val=loss_a(adj_recon, adj_label, pos_weight, norm,val_nodes_idx)\n",
    "\n",
    "            loss_val=loss_x_val\n",
    "            loss_val=loss_val+loss_a_val\n",
    "\n",
    "            print(training_samples_t+' Epoch: {:04d}'.format(epoch),\n",
    "                  'loss_train: {:.4f}'.format(loss.item()),\n",
    "                  'loss_kl_train: {:.4f}'.format(loss_kl_train.item()),\n",
    "                  'loss_x_train: {:.4f}'.format(loss_x_train.item()),\n",
    "                  'loss_a_train: {:.4f}'.format(loss_a_train.item()),\n",
    "                  'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "                  'loss_x_val: {:.4f}'.format(loss_x_val.item()),\n",
    "                  'loss_a_val: {:.4f}'.format(loss_a_val.item()),\n",
    "                  'time: {:.4f}s'.format(time.time() - t))\n",
    "\n",
    "            return loss.item(),loss_kl_train.item(),loss_x_train.item(),loss_a_train.item(),loss_val.item(),loss_x_val.item(),loss_a_val.item()        \n",
    "\n",
    "        # print('cross-validation ',seti)\n",
    "        train_loss_ep=[None]*epochs\n",
    "        train_loss_kl_ep=[None]*epochs\n",
    "        train_loss_x_ep=[None]*epochs\n",
    "        train_loss_a_ep=[None]*epochs\n",
    "        train_loss_adv_ep=[None]*epochs\n",
    "        train_loss_advD_ep=[None]*epochs\n",
    "        val_loss_ep=[None]*epochs\n",
    "        val_loss_x_ep=[None]*epochs\n",
    "        val_loss_a_ep=[None]*epochs\n",
    "        val_loss_adv_ep=[None]*epochs\n",
    "        val_loss_advD_ep=[None]*epochs\n",
    "        t_ep=time.time()\n",
    "\n",
    "        for ep in range(epochs):\n",
    "            t=int(ep/switchFreq)%len(training_samples)\n",
    "            training_samples_t=training_samples[t]\n",
    "\n",
    "            adj_norm=adjnormlist[training_samples_t].cuda().float()\n",
    "            adj_label=adj_list[training_samples_t].cuda().float()\n",
    "            features=featureslist[training_samples_t+'X_logminmax'].cuda().float()\n",
    "            pos_weight=pos_weightlist[training_samples_t]\n",
    "            norm=normlist[training_samples_t]\n",
    "            features_raw=features_raw_list[training_samples_t+'X_raw'].cuda()\n",
    "            num_nodes,_ = features.shape\n",
    "\n",
    "            maskpath=os.path.join(savedir,'trainMask',training_samples_t+'_'+maskedgeName+'_seed'+str(model_seeds[run_number-1])+'.pkl')\n",
    "            if useSavedMaskedEdges and os.path.exists(maskpath):\n",
    "                with open(maskpath, 'rb') as input:\n",
    "                    maskedgeres = pickle.load(input)\n",
    "            else:\n",
    "                # construct training, validation, and test sets\n",
    "                maskedgeres= preprocessing.mask_nodes_edges(features.shape[0],testNodeSize=testNodes,valNodeSize=valNodes,seed=model_seeds[run_number-1])\n",
    "                os.makedirs(savedir+\"/trainMask\", exist_ok=True)\n",
    "                with open(maskpath, 'wb') as output:\n",
    "                    pickle.dump(maskedgeres, output, pickle.HIGHEST_PROTOCOL)\n",
    "            train_nodes_idx,val_nodes_idx,test_nodes_idx = maskedgeres\n",
    "            if use_cuda:\n",
    "                train_nodes_idx=train_nodes_idx.cuda()\n",
    "                val_nodes_idx=val_nodes_idx.cuda()\n",
    "                test_nodes_idx=test_nodes_idx.cuda()\n",
    "\n",
    "            train_loss_ep[ep],train_loss_kl_ep[ep],train_loss_x_ep[ep],train_loss_a_ep[ep],val_loss_ep[ep],val_loss_x_ep[ep],val_loss_a_ep[ep]=train(ep)\n",
    "\n",
    "            if ep%saveFreq == 0:\n",
    "                torch.save(model.cpu().state_dict(), os.path.join(modelsavepath,str(ep)+'.pt'))\n",
    "            if use_cuda:\n",
    "                model.cuda()\n",
    "                torch.cuda.empty_cache()\n",
    "        print(' total time: {:.4f}s'.format(time.time() - t_ep))\n",
    "        \n",
    "        # Measure time for model training\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "        hours, rem = divmod(elapsed_time, 3600)\n",
    "        minutes, seconds = divmod(rem, 60)\n",
    "        print(f\"Duration of model training in run {run_number}: \"\n",
    "              f\"{int(hours)} hours, {int(minutes)} minutes and {int(seconds)} seconds.\")\n",
    "        adata_new.uns[f\"{model_name}_model_training_duration_run{run_number}\"] = (\n",
    "            elapsed_time)\n",
    "            \n",
    "        # Store latent representation in adata\n",
    "        mu_all_samples = np.zeros((len(adata), hidden2))\n",
    "        n_obs_start = 0\n",
    "        for training_sample in training_samples:\n",
    "            adj_norm=adjnormlist[training_sample].cuda().float()\n",
    "            features=featureslist[training_sample+'X_logminmax'].cuda().float()\n",
    "            n_obs_end = n_obs_start + len(features)\n",
    "            adj_recon,mu,logvar,z,features_recon = model(features, adj_norm)\n",
    "            mu_all_samples[n_obs_start:n_obs_end] = mu.cpu().detach().numpy()  \n",
    "            n_obs_start = n_obs_end\n",
    "        adata_new.obsm[latent_key + f\"_run{run_number}\"] = mu_all_samples\n",
    "\n",
    "        # Store intermediate adata to disk\n",
    "        adata_new.write(f\"{benchmarking_folder_path}/{dataset}_{model_name}.h5ad\")\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    # Store final adata to disk\n",
    "    adata_new.write(f\"{benchmarking_folder_path}/{dataset}_{model_name}.h5ad\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Train Models on Benchmarking Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_staci_models(dataset=\"seqfish_mouse_organogenesis\",\n",
    "                   reference_batches=[f\"batch{i}\" for i in range(1,7)],\n",
    "                   cell_type_key=\"celltype_mapped_refined\",\n",
    "                   adata_new=None,\n",
    "                   n_start_run=1,\n",
    "                   n_end_run=8,\n",
    "                   n_neighbor_list=[4, 4, 8, 8, 12, 12, 16, 16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for subsample_pct in [50, 25, 10, 5, 1]:\n",
    "    train_staci_models(dataset=f\"seqfish_mouse_organogenesis_subsample_{subsample_pct}pct\",\n",
    "                       reference_batches=[f\"batch{i}\" for i in range(1,7)],\n",
    "                       cell_type_key=\"celltype_mapped_refined\",\n",
    "                       adata_new=None,\n",
    "                       n_start_run=1,\n",
    "                       n_end_run=8,\n",
    "                       n_neighbor_list=[4, 4, 8, 8, 12, 12, 16, 16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_staci_models(dataset=\"seqfish_mouse_organogenesis_imputed\",\n",
    "                   reference_batches=[f\"batch{i}\" for i in range(1,7)],\n",
    "                   cell_type_key=\"celltype_mapped_refined\",\n",
    "                   adata_new=None,\n",
    "                   n_start_run=1,\n",
    "                   n_end_run=8,\n",
    "                   n_neighbor_list=[4, 4, 8, 8, 12, 12, 16, 16],\n",
    "                   filter_genes=True,\n",
    "                   n_svg=3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for subsample_pct in [50, 25, 10, 5, 1]:\n",
    "    train_staci_models(dataset=f\"seqfish_mouse_organogenesis_imputed_subsample_{subsample_pct}pct\",\n",
    "                       reference_batches=[f\"batch{i}\" for i in range(1,7)],\n",
    "                       cell_type_key=\"celltype_mapped_refined\",\n",
    "                       adata_new=None,\n",
    "                       n_start_run=1,\n",
    "                       n_end_run=8,\n",
    "                       n_neighbor_list=[4, 4, 8, 8, 12, 12, 16, 16],\n",
    "                       filter_genes=True,\n",
    "                       n_svg=3000)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# This does not work because of memory exhaustion\n",
    "# NVIDIA A100-PCIE-40GB\n",
    "train_staci_models(dataset=\"nanostring_cosmx_human_nsclc\",\n",
    "                   reference_batches=[f\"batch{i}\" for i in range(1,4)],\n",
    "                   cell_type_key=\"cell_type\",\n",
    "                   adata_new=None,\n",
    "                   n_start_run=1,\n",
    "                   n_end_run=8,\n",
    "                   n_neighbor_list=[4, 4, 8, 8, 12, 12, 16, 16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for subsample_pct in [10, 5, 1]: # 50, 25 pct exhaust memory\n",
    "    train_staci_models(dataset=f\"nanostring_cosmx_human_nsclc_subsample_{subsample_pct}pct\",\n",
    "                       reference_batches=[f\"batch{i}\" for i in range(1,4)],\n",
    "                       cell_type_key=\"cell_type\",\n",
    "                       adata_new=None,\n",
    "                       n_start_run=1,\n",
    "                       n_end_run=8,\n",
    "                       n_neighbor_list=[4, 4, 8, 8, 12, 12, 16, 16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
